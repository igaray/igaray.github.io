<!DOCTYPE html>
<html lang="en">
    <head>
        <!-- Global Site Tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-59055167-2"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments)};
          gtag('js', new Date());

          gtag('config', 'UA-59055167-2');
        </script>

        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        <link rel="canonical" href="https://notes.shichao.io/dda/ch3/">
        <link rel="shortcut icon" href="../../toki_32.png">
        

	<title>Chatper 3. Storage and Retrieval - Shichao's Notes</title>

        <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../css/font-awesome-4.0.3.css" rel="stylesheet">
        <link rel="stylesheet" href="../../css/highlight.css">
        <link href="../../css/base.css" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,400italic,500,600" rel="stylesheet">
        <link href="../../custom.css" rel="stylesheet">
        <link href="../../friendly.css" rel="stylesheet">
        <link href="../../theme.css" rel="stylesheet">

        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        
    </head>

    <body>

        
<div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <!-- Main title -->
            <a class="navbar-brand" href="../..">Shichao's Notes</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
            <!-- Main navigation -->
            <ul class="nav navbar-nav">
            <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">APUE <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        <li>
                            <a href="../../apue/">Contents</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch1/">Chapter 1. UNIX System Overview</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch2/">Chapter 2. UNIX Standardization and Implementations</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch3/">Chapter 3. File I/O</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch4/">Chapter 4. Files and Directories</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch5/">Chapter 5. Standard I/O Library</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch6/">Chapter 6. System Data Files and Information</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch7/">Chapter 7. Process Environment</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch8/">Chapter 8. Process Control</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch9/">Chapter 9. Process Relationships</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch10/">Chapter 10. Signals</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch11/">Chapter 11. Threads</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch12/">Chapter 12. Thread Control</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch13/">Chapter 13. Daemon Processes</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch14/">Chapter 14. Advanced I/O</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch15/">Chapter 15. Interprocess Communication</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch16/">Chapter 16. Network IPC: Sockets</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch17/">Chapter 17. Advanced IPC</a>
                        </li>
                      
                    </ul>
                </li>
            <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">UNP <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        <li>
                            <a href="../../unp/">Contents</a>
                        </li>
                      
                        <li>
                            <a href="../../unp/ch1/">Chapter 1. Introduction</a>
                        </li>
                      
                        <li>
                            <a href="../../unp/ch2/">Chapter 2. The Transport Layer: TCP, UDP, and SCTP</a>
                        </li>
                      
                        <li>
                            <a href="../../unp/ch3/">Chapter 3. Sockets Introduction</a>
                        </li>
                      
                        <li>
                            <a href="../../unp/ch4/">Chapter 4. Elementary TCP Sockets</a>
                        </li>
                      
                        <li>
                            <a href="../../unp/ch5/">Chapter 5. TCP Client/Server Example</a>
                        </li>
                      
                        <li>
                            <a href="../../unp/ch6/">Chapter 6. I/O Multiplexing: The select and poll Functions</a>
                        </li>
                      
                        <li>
                            <a href="../../unp/ch7/">Chapter 7. Socket Options</a>
                        </li>
                      
                        <li>
                            <a href="../../unp/ch8/">Chapter 8. Elementary UDP Sockets</a>
                        </li>
                      
                    </ul>
                </li>
            <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">TCPv1 <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        <li>
                            <a href="../../tcpv1/">Contents</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch1/">Chapter 1. Introduction</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch2/">Chapter 2. The Internet Address Architecture</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch3/">Chapter 3. Link Layer</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch4/">Chapter 4. ARP: Address Resolution Protocol</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch5/">Chapter 5. The Internet Protocol (IP)</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch6/">Chapter 6. System Configuration: DHCP and Autoconfiguration</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch7/">Chapter 7. Firewalls and Network Address Translation (NAT)</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch8/">Chapter 8. ICMPv4 and ICMPv6: Internet Control Message Protocol</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch9/">Chapter 9. Broadcasting and Local Multicasting (IGMP and MLD)</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch10/">Chapter 10. User Datagram Protocol (UDP) and IP Fragmentation</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch11/">Chapter 11. Name Resolution and the Domain Name System (DNS)</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch12/">Chapter 12. TCP: The Transmission Control Protocol (Preliminaries)</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch13/">Chapter 13. TCP Connection Management</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch14/">Chapter 14. TCP Timeout and Retransmission</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch15/">Chapter 15. TCP Data Flow and Window Management</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch16/">Chapter 16. TCP Congestion Control</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch17/">Chapter 17. TCP Keepalive</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch18/">Chapter 18. Security: EAP, IPsec, TLS, DNSSEC, and DKIM</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/headers/">Headers</a>
                        </li>
                      
                    </ul>
                </li>
            <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">GOPL <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        <li>
                            <a href="../../gopl/">Contents</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch1/">Chapter 1. Tutorial</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch2/">Chapter 2. Program Structure</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch3/">Chapter 3. Basic Data Types</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch4/">Chapter 4. Composite Types</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch5/">Chapter 5. Functions</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch6/">Chapter 6. Methods</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch7/">Chapter 7. Interfaces</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch8/">Chapter 8. Goroutines and Channels</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch9/">Chapter 9. Concurrency with Shared Variables</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch10/">Chapter 10. Packages and the Go Tool</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch11/">Chapter 11. Testing</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch12/">Chapter 12. Reflection</a>
                        </li>
                      
                    </ul>
                </li>
            <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">CSN <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        <li>
                            <a href="../../csn/">Contents</a>
                        </li>
                      
                        <li>
                            <a href="../../csn/part1/">Part 1: Language</a>
                        </li>
                      
                        <li>
                            <a href="../../csn/part2/">Part 2: Advanced</a>
                        </li>
                      
                    </ul>
                </li>
            <li>
                    <a href="../../toc/">TOC</a>
                </li>
            </ul>
            <!-- Search, Navigation and Repo links -->
            <ul class="nav navbar-nav navbar-right">
                
                <li>
                    
                        <a href="https://github.com/shichao-an/notes/blob/master/docs/dda/ch3.md">
                    
                        
                            <i class="fa fa-github"></i>
                        
                        GitHub
                    </a>
                </li>
                
            </ul>
        </div>
    </div>
</div>

        <div class="container">
            <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
    
        <li class="main active"><a href="#chatper-3-storage-and-retrieval">Chatper 3. Storage and Retrieval</a></li>
        
    
        <li class="main "><a href="#data-structures-that-power-your-database">Data Structures That Power Your Database</a></li>
        
            <li><a href="#hash-indexes">Hash Indexes</a></li>
        
            <li><a href="#sstables-and-lsm-trees">SSTables and LSM-Trees</a></li>
        
            <li><a href="#b-trees">B-Trees</a></li>
        
            <li><a href="#comparing-b-trees-and-lsm-trees">Comparing B-Trees and LSM-Trees</a></li>
        
            <li><a href="#other-indexing-structures">Other Indexing Structures</a></li>
        
    
    </ul>
</div></div>
            <div class="col-md-9" role="main">
              

<h3 id="chatper-3-storage-and-retrieval"><strong>Chatper 3. Storage and Retrieval</strong><a class="headerlink" href="#chatper-3-storage-and-retrieval" title="Permanent link">&para;</a></h3>
<p>On the most fundamental level, a database needs to do two things:</p>
<ol>
<li>When you give it some data, it should store the data.</li>
<li>When you ask it again later, it should give the data back to you.</li>
</ol>
<p><a href="../ch2/">Chapter 2</a> discussed data models and query languages, the format in which you (the application developer) give the database your data, and the mechanism by which you can ask for it again later.</p>
<p>This chapter is from the database's point of view: how we can store the data that we're given, and how we can find it again when we're asked for it.</p>
<p>As an application developer, you are probably not going to implement your own storage engine from scratch, but you do need to select a storage engine that is appropriate for your application. You need to care about how the database handles storage and retrieval internally.</p>
<p>In particular, there is a big difference between:</p>
<ul>
<li>Storage engines that are optimized for transactional workloads: see <a href="#transaction-processing-or-analytics">Transaction Processing or Analytics?</a></li>
<li>Storage engines that are optimized for analytics: see <a href="#column-oriented-storage">Column-Oriented Storage</a></li>
</ul>
<h3 id="data-structures-that-power-your-database">Data Structures That Power Your Database<a class="headerlink" href="#data-structures-that-power-your-database" title="Permanent link">&para;</a></h3>
<p>Consider the world's simplest database, implemented as two Bash functions:</p>
<div class="codehilite"><pre><span class="ch">#!/bin/bash</span>

db_set <span class="o">()</span> <span class="o">{</span>
    <span class="nb">echo</span> <span class="s2">&quot;</span><span class="nv">$1</span><span class="s2">,</span><span class="nv">$2</span><span class="s2">&quot;</span> &gt;&gt; database
<span class="o">}</span>

db_get <span class="o">()</span> <span class="o">{</span>
    grep <span class="s2">&quot;^</span><span class="nv">$1</span><span class="s2">,&quot;</span> database <span class="p">|</span> sed -e <span class="s2">&quot;s/^</span><span class="nv">$1</span><span class="s2">,//&quot;</span> <span class="p">|</span> tail -n 1
<span class="o">}</span>
</pre></div>


<p>These two functions implement a key-value store:</p>
<ul>
<li>You can call <code>db_set</code> key value, which will store key and value in the database. The key and value can be anything you like, e.g., the value could be a JSON document.</li>
<li>You can then call <code>db_get</code> key, which looks up the most recent value associated with that particular key and returns it.</li>
</ul>
<div class="codehilite"><pre><span class="gp">$</span> db_set <span class="m">123456</span> <span class="s1">&#39;{&quot;name&quot;:&quot;London&quot;,&quot;attractions&quot;:[&quot;Big Ben&quot;,&quot;London Eye&quot;]}&#39;</span>
<span class="gp">$</span> db_set <span class="m">42</span> <span class="s1">&#39;{&quot;name&quot;:&quot;San Francisco&quot;,&quot;attractions&quot;:[&quot;Golden Gate Bridge&quot;]}&#39;</span>
<span class="gp">$</span> db_get 42
<span class="go">{&quot;name&quot;:&quot;San Francisco&quot;,&quot;attractions&quot;:[&quot;Golden Gate Bridge&quot;]}</span>
</pre></div>


<p>The underlying storage format is very simple: a text file where each line contains a key-value pair, separated by a comma (roughly like a <a href="https://en.wikipedia.org/wiki/Comma-separated_values">CSV</a> file, ignoring escaping issues). Every call to <code>db_set</code> appends to the end of the file, so if you update a key several times, the old versions of the value are not overwritten, you need to look at the last occurrence of a key in a file to find the latest value (hence the <code>tail -n 1</code> in <code>db_get</code>):</p>
<p>This <code>db_set</code> function actually has pretty good performance, because appending to a file is generally very efficient. Similarly to what <code>db_set</code> does, many databases internally use a log, which is an append-only data file. Real databases have more issues to deal with, for example:</p>
<ul>
<li>Concurrency control</li>
<li>Reclaiming disk space so that the log doesn't grow forever</li>
<li>Handling errors and partially written records</li>
</ul>
<blockquote>
<p><small>The word <em>log</em> is often used to refer to application logs, where an application outputs text that describes what's happening. In this book, <em>log</em> is used in the more general sense: an append-only sequence of records. It doesn't have to be human-readable; it might be binary and intended only for other programs to read.</small></p>
</blockquote>
<p>On the other hand, the <code>db_get</code> function has terrible performance if you have a large number of records in your database. <code>db_get</code> has to scan the entire database file from beginning to end, looking for occurrences of the key. In algorithmic terms, the cost of a lookup is <em>O(n)</em>.</p>
<p>In order to efficiently find the value for a particular key in the database, we need a different data structure: an <em>index</em>. The general idea behind index is to keep some additional metadata on the side, which acts as a signpost and helps you to locate the data you want. If you want to search the same data in several different ways, you may need several different indexes on different parts of the data.</p>
<p>An index is an additional structure that is derived from the primary data. Many databases allow you to add and remove indexes, and this doesn't affect the contents of the database; it only affects the performance of queries. Maintaining additional structures incurs overhead, especially on writes. For writes, it's hard to beat the performance of simply appending to a file, because that's the simplest possible write operation. Any kind of index usually slows down writes, because the index also needs to be updated every time data is written.</p>
<p><u>This is an important trade-off in storage systems: well-chosen indexes speed up read queries, but every index slows down writes.</u> For this reason, databases don't usually index everything by default, but require you (the application developer or database administrator) to choose indexes manually, using your knowledge of the application's typical query patterns. You can then choose the indexes that give your application the greatest benefit, without introducing more overhead than necessary.</p>
<h4 id="hash-indexes">Hash Indexes<a class="headerlink" href="#hash-indexes" title="Permanent link">&para;</a></h4>
<p>Although key-value data is not the only kind of data you can index, but it's very common, and it's a useful building block for more complex indexes.</p>
<p>Key-value stores are quite similar to the <a href="https://en.wikipedia.org/wiki/Associative_array"><em>dictionary</em></a> type that you can find in most programming languages, and which is usually implemented as a <a href="https://en.wikipedia.org/wiki/Hash_table">hash map</a> (hash table).</p>
<p>In the preceding example, our data storage consists only of appending to a file. Then the simplest possible indexing strategy is this: keep an in-memory hash map where every key is mapped to a byte offset in the data file, as illustrated in <a href="../figure_3-1.png">Figure 3-1</a>.</p>
<ul>
<li>Whenever you append a new key-value pair to the file, you also update the hash map to reflect the offset of the data you just wrote (this works both for inserting new keys and for updating existing keys).</li>
<li>When you want to look up a value, use the hash map to find the offset in the data file, seek to that location, and read the value.</li>
</ul>
<p><a href="../figure_3-1.png" title="Figure 3-1. Storing a log of key-value pairs in a CSV-like format, indexed with an in-memory hash map."><img alt="Figure 3-1. Storing a log of key-value pairs in a CSV-like format, indexed with an in-memory hash map." src="../figure_3-1_600.png" /></a></p>
<p>This may sound simplistic, but it is a viable approach. In fact, this is essentially what <a href="https://en.wikipedia.org/wiki/Bitcask">Bitcask</a> (the default storage engine in <a href="https://en.wikipedia.org/wiki/Riak">Riak</a>) does:</p>
<ul>
<li>Bitcask offers high-performance reads and writes, subject to the requirement that all the keys fit in the available RAM, since the hash map is kept completely in memory.</li>
<li>The values can use more space than there is available memory, since they can be loaded from disk with just one disk seek.</li>
<li>If part of the data file is already in the filesystem cache, a read doesn't require any disk I/O at all.</li>
</ul>
<p>A storage engine like Bitcask is well suited to situations where the value for each key is updated frequently. For example, the key is an URL of a cat video and the value might be the number of times it has been played. In this kind of workload, there are a lot of writes, but there are not too many distinct keys. Although you have a large number of writes per key, but it's feasible to keep all keys in memory.</p>
<p>So far we only ever append to a file. How do we avoid eventually running out of disk space? A good solution is to break the log into segments of a certain size by closing a segment file when it reaches a certain size, and making subsequent writes to a new segment file. We can then perform <em>compaction</em> on these segments, as illustrated in <a href="../figure_3-2.png">Figure 3-2</a>. Compaction means throwing away duplicate keys in the log, and keeping only the most recent update for each key.</p>
<p><a href="../figure_3-2.png" title="Figure 3-2. Compaction of a key-value update log (counting the number of times each cat video was played), retaining only the most recent value for each key"><img alt="Figure 3-2. Compaction of a key-value update log (counting the number of times each cat video was played), retaining only the most recent value for each key" src="../figure_3-2_600.png" /></a></p>
<p>Moreover, since compaction often makes segments much smaller (assuming that a key is overwritten several times on average within one segment), we can also merge several segments together at the same time as performing the compaction, as shown in <a href="../figure_3-3.png">Figure 3-3</a>. Segments are never modified after they have been written, so the merged segment is written to a new file. The merging and compaction of frozen segments can be done in a background thread, and while it is going on, we can still continue to serve read and write requests as normal, using the old segment files. After the merging process is complete, we switch read requests to using the new merged segment instead of the old segments, and then the old segment files can simply be deleted.</p>
<p><a href="../figure_3-3.png" title="Figure 3-3. Performing compaction and segment merging simultaneously."><img alt="Figure 3-3. Performing compaction and segment merging simultaneously." src="../figure_3-3_600.png" /></a></p>
<p>Each segment now has its own in-memory hash table, mapping keys to file offsets. In order to find the value for a key, we first check the most recent segment's hash map; if the key is not present we check the second-most-recent segment, and so on. The merging process keeps the number of segments small, so lookups don't need to check many hash maps.</p>
<p>Some of the issues that are important in a real implementation are:</p>
<ul>
<li><strong>File format</strong>. CSV is not the best format for a log. It's faster and simpler to use a binary format that first encodes the length of a string in bytes, followed by the raw string (without need for escaping).</li>
<li><strong>Deleting records</strong>. If you want to delete a key and its associated value, you have to append a special deletion record to the data file (sometimes called a <a href="https://en.wikipedia.org/wiki/Tombstone_(data_store)"><em>tombstone</em></a>). When log segments are merged, the tombstone tells the merging process to discard any previous values for the deleted key.</li>
<li><strong>Crash recovery</strong>. If the database is restarted, the in-memory hash maps are lost. In principle, you can restore each segment's hash map by reading the entire segment file from beginning to end and noting the offset of the most recent value for every key as you go along. However, that might take a long time if the segment files are large, which would make server restarts painful. Bitcask speeds up recovery by storing a snapshot of each segment's hash map on disk, which can be loaded into memory more quickly.</li>
<li><strong>Partially written records</strong>. The database may crash at any time, including halfway through appending a record to the log. Bitcask files include checksums, allowing such corrupted parts of the log to be detected and ignored.</li>
<li><strong>Concurrency control</strong>. As writes are appended to the log in a strictly sequential order, a common implementation choice is to have only one writer thread. Data file segments are append-only and otherwise immutable, so they can be read concurrently by multiple threads.</li>
</ul>
<p>An append-only log seems wasteful at first glance: why don't you update the file in place, overwriting the old value with the new value?</p>
<p>The append-only design turns out to be good for several reasons:</p>
<ul>
<li>Appending and segment merging are sequential write operations, which are generally much faster than random writes, especially on magnetic spinning-disk hard drives. To some extent sequential writes are also preferable on flash-based solid state drives (SSDs). See further discussions <a href="#comparing-btrees-and-lsm-trees">Comparing BTrees and LSM-Trees</a>.</li>
<li>Concurrency and crash recovery are much simpler if segment files are append-only or immutable. For example, you don't have to worry about the case where a crash happened while a value was being overwritten, leaving you with a file containing part of the old and part of the new value spliced together.</li>
<li>Merging old segments avoids the problem of data files getting fragmented over time.</li>
</ul>
<p>However, the hash table index also has limitations:</p>
<ul>
<li>The hash table must fit in memory, so if you have a very large number of keys, you're out of luck. In principle, you could maintain a hash map on disk, but unfortunately it is difficult to make an on-disk hash map perform well. It requires a lot of random access I/O, it is expensive to grow when it becomes full, and hash collisions require fiddly logic.</li>
<li><a href="https://en.wikipedia.org/wiki/Range_query_(database)">Range queries</a> are not efficient. For example, you cannot easily scan over all keys between <code>kitty00000</code> and <code>kitty99999</code>. You'd have to look up each key individually in the hash maps.</li>
</ul>
<p>The indexing structure in the next section doesn't have those limitations.</p>
<h4 id="sstables-and-lsm-trees">SSTables and LSM-Trees<a class="headerlink" href="#sstables-and-lsm-trees" title="Permanent link">&para;</a></h4>
<p>In <a href="../figure_3-3.png">Figure 3-3</a>, each log-structured storage segment is a sequence of key-value pairs. These pairs appear in the order that they were written, and values later in the log take precedence over values for the same key earlier in the log. Apart from that, the order of key-value pairs in the file does not matter.</p>
<p>Now we can make a simple change to the format of our segment files: we require that the sequence of key-value pairs is <em>sorted by key</em>. At first glance, that requirement seems to break our ability to use sequential writes (to be discussed later).</p>
<p>We call this format <em>Sorted String Table</em>, or <em>SSTable</em> for short. We also require that each key only appears once within each merged segment file (the compaction process already ensures that).</p>
<p>SSTables have several big advantages over log segments with hash indexes:</p>
<p><a href="../figure_3-4.png" title="Figure 3-4. Merging several SSTable segments, retaining only the most recent value for each key."><img alt="Figure 3-4. Merging several SSTable segments, retaining only the most recent value for each key." src="../figure_3-4_600.png" /></a></p>
<ol>
<li>Merging segments is simple and efficient, even if the files are bigger than the available memory. The approach is like the one used in the <a href="https://en.wikipedia.org/wiki/Merge_sort"><em>mergesort</em></a> algorithm and is illustrated in <a href="../figure_3-4.png">Figure 3-4</a> (above): read the input files side by side, for the first key in each file, copy the lowest key (according to the sort order) to the output file, and repeat. This produces a new merged segment file, also sorted by key. When multiple segments contain the same key, keep the value from the most recent segment and discard the values in older segments. [p77]</li>
<li>In order to find a particular key in the file, you no longer need to keep an index of all the keys in memory. See <a href="../figure_3-5.png">Figure 3-5</a> (below) for an example: you're looking for the key <code>handiwork</code>, but you don't know the exact offset of that key in the segment file. However, you do know the offsets for the keys <code>handbag</code> and <code>handsome</code>, and because of the sorting you know that <code>handiwork</code> must appear between those two. This means you can jump to the offset for <code>handbag</code> and scan from there until you find <code>handiwork</code> (or not, if the key is not present in the file).<ul>
<li>You still need an in-memory index to tell you the offsets for some of the keys, but it can be sparse: one key for every few kilobytes of segment file is sufficient, because a few kilobytes can be scanned very quickly.</li>
<li>If all keys and values had a fixed size, you could use binary search on a segment file and avoid the in-memory index entirely. However, they are usually variable-length in practice, which makes it difficult to tell where one record ends and the next one starts if you don't have an index.</li>
</ul>
</li>
<li>Since read requests need to scan over several key-value pairs in the requested range anyway, it is possible to group those records into a block and compress it before writing it to disk (indicated by the shaded area in <a href="../figure_3-5.png">Figure 3-5</a>). Each entry of the sparse in-memory index then points at the start of a compressed block.  Besides saving disk space, compression also reduces the I/O bandwidth use.</li>
</ol>
<p><a href="../figure_3-5.png" title="Figure 3-5. An SSTable with an in-memory index."><img alt="Figure 3-5. An SSTable with an in-memory index." src="../figure_3-5_600.png" /></a></p>
<h5 id="constructing-and-maintaining-sstables"><strong>Constructing and maintaining SSTables</strong><a class="headerlink" href="#constructing-and-maintaining-sstables" title="Permanent link">&para;</a></h5>
<p>How do you get your data to be sorted by key in the first place? Incoming writes can occur in any order.</p>
<p>Maintaining a sorted structure on disk is possible (see <a href="#b-trees">B-Trees</a> in the upcoming section), but maintaining it in memory is much easier. With a plenty of well-known tree data structures you can use, such as <a href="https://en.wikipedia.org/wiki/Red%E2%80%93black_tree">red-black trees</a> or <a href="https://en.wikipedia.org/wiki/AVL_tree">AVL trees</a>, you can insert keys in any order and read them back in sorted order.</p>
<p>We can now make our storage engine work as follows:</p>
<ul>
<li>When a write comes in, add it to an in-memory balanced tree data structure (for example, a red-black tree). This in-memory tree is sometimes called a <em>memtable</em>.</li>
<li>When the memtable gets bigger than some threshold (typically a few megabytes) write it out to disk as an SSTable file. This can be done efficiently because the tree already maintains the key-value pairs sorted by key. The new SSTable file becomes the most recent segment of the database. While the SSTable is being written out to disk, writes can continue to a new memtable instance.</li>
<li>In order to serve a read request, first try to find the key in the memtable, then in the most recent on-disk segment, then in the next-older segment, etc.</li>
<li>From time to time, run a merging and compaction process in the background to combine segment files and to discard overwritten or deleted values.</li>
</ul>
<p>This works very well except for one problem: if the database crashes, the most recent writes (which are in the memtable but not yet written out to disk) are lost. In order to avoid that problem, we can keep a separate log on disk to which every write is immediately appended, just like in the previous section. That log is not in sorted order, but that doesn't matter, because its only purpose is to restore the memtable after a crash. Every time the memtable is written out to an SSTable, the corresponding log can be discarded.</p>
<h5 id="making-an-lsm-tree-out-of-sstables"><strong>Making an LSM-tree out of SSTables</strong><a class="headerlink" href="#making-an-lsm-tree-out-of-sstables" title="Permanent link">&para;</a></h5>
<p>The algorithm described here is essentially what is used in <a href="https://en.wikipedia.org/wiki/LevelDB">LevelDB</a> and <a href="https://en.wikipedia.org/wiki/RocksDB">RocksDB</a>, key-value storage engine libraries that are designed to be embedded into other applications. Among other things, LevelDB can be used in Riak as an alternative to Bitcask. Similar storage engines are used in <a href="https://en.wikipedia.org/wiki/Apache_Cassandra">Cassandra</a> and <a href="https://en.wikipedia.org/wiki/Apache_HBase">HBase</a>, both of which were inspired by Google's <a href="https://en.wikipedia.org/wiki/Bigtable">Bigtable</a> paper.</p>
<p>Originally this indexing structure was described by <a href="https://en.wikipedia.org/wiki/Patrick_O%27Neil">Patrick O'Neil</a> et al. under the name <a href="https://en.wikipedia.org/wiki/Log-structured_merge-tree">Log-Structured Merge-Tree</a> (or LSM-Tree), building on earlier work on <a href="https://en.wikipedia.org/wiki/Log-structured_file_system">log-structured filesystems</a>. Storage engines that are based on this principle of merging and compacting sorted files are often called LSM storage engines.</p>
<p><a href="https://en.wikipedia.org/wiki/Apache_Lucene">Lucene</a>, an indexing engine for full-text search used by <a href="https://en.wikipedia.org/wiki/Elasticsearch">Elasticsearch</a> and <a href="https://en.wikipedia.org/wiki/Apache_Solr">Solr</a>, uses a similar method for storing its <em>term dictionary</em>. A full-text index is much more complex than a key-value index but is based on a similar idea: given a word in a search query, find all the documents (web pages, product descriptions, etc.) that mention the word. This is implemented with a key-value structure where the key is a word (a <em>term</em>) and the value is the list of IDs of all the documents that contain the word (the postings list). In Lucene, this mapping from term to postings list is kept in SSTable-like sorted files, which are merged in the background as needed.</p>
<h5 id="performance-optimizations"><strong>Performance optimizations</strong><a class="headerlink" href="#performance-optimizations" title="Permanent link">&para;</a></h5>
<p>As always, a lot of detail goes into making a storage engine perform well in practice. For example, the LSM-tree algorithm can be slow when looking up keys that do not exist in the database: you have to check the memtable, then the segments all the way back to the oldest before you can be sure that the key does not exist. In order to optimize this kind of access, storage engines often use additional <a href="https://en.wikipedia.org/wiki/Bloom_filter">Bloom filters</a>. (A Bloom filter is a memory-efficient data structure for approximating the contents of a set. It can tell you if a key does not appear in the database, and thus saves many unnecessary disk reads for nonexistent keys.)</p>
<p>There are also different strategies to determine the order and timing of how SSTables are compacted and merged. The most common options are <em>size-tiered</em> and <em>leveled</em> compaction. LevelDB and RocksDB use leveled compaction (hence the name of LevelDB), HBase uses size-tiered, and Cassandra supports both.</p>
<ul>
<li>In <strong>size-tiered compaction</strong>, newer and smaller SSTables are successively merged into older and larger SSTables. (See <a href="https://academy.datastax.com/resources/ds210-datastax-enterprise-operations-apache-cassandra?unit=size-tiered-compaction">DS210 Size Tiered Compaction</a>)</li>
<li>In <strong>leveled compaction</strong>, the key range is split up into smaller SSTables and older data is moved into separate "levels", which allows the compaction to proceed more incrementally and use less disk space. (See <a href="https://academy.datastax.com/resources/ds210-datastax-enterprise-operations-apache-cassandra?unit=leveled-compaction">DS210 Leveled Compaction</a>)</li>
</ul>
<p>Even though there are many subtleties, the basic idea of LSM-trees is keeping a cascade of SSTables that are merged in the background, which is simple and effective. Even when the dataset is much bigger than the available memory it continues to work well. Since data is stored in sorted order, you can efficiently perform range queries (scanning all keys above some minimum and up to some maximum), and because the disk writes are sequential the LSM-tree can support remarkably high write throughput.</p>
<h4 id="b-trees">B-Trees<a class="headerlink" href="#b-trees" title="Permanent link">&para;</a></h4>
<p>Though gaining acceptance, the log-structured indexes are not the most common type of index. The most widely used indexing structure is quite different: the <a href="https://en.wikipedia.org/wiki/B-tree"><em>B-tree</em></a>.</p>
<p>Introduced in 1970 and called "ubiquitous" less than 10 years later, B-trees remain the standard index implementation in almost all relational databases, and also used by many non-relational databases.</p>
<p>Like SSTables, B-trees keep key-value pairs sorted by key, which allows efficient key-value lookups and range queries. But that's where the similarity ends: B-trees have a very different design philosophy.</p>
<p>The log-structured indexes break the database down into variable-size <em>segments</em>, typically several megabytes or more in size, and always write a segment sequentially. By contrast, B-trees break the database down into fixed-size <em>blocks</em> or <em>pages</em>, traditionally 4 KB in size (sometimes bigger), and read or write one page at a time. This design corresponds more closely to the underlying hardware, as disks are also arranged in fixed-size blocks.</p>
<p>Each page can be identified using an address or location, which allows one page to refer to another (similar to a pointer, but on disk instead of in memory). We can use these page references to construct a tree of pages, as illustrated in <a href="../figure_3-6.png">Figure 3-6</a> (below).</p>
<p><a href="../figure_3-6.png" title="Figure 3-6. Looking up a key using a B-tree index."><img alt="Figure 3-6. Looking up a key using a B-tree index." src="../figure_3-6_600.png" /></a></p>
<ul>
<li>One page is designated as the <em>root</em> of the B-tree, which is where you starts your lookup.</li>
<li>The page contains several keys and references to child pages.</li>
<li>Each child is responsible for a continuous range of keys, and the keys between the references indicate where the boundaries between those ranges lie.</li>
</ul>
<p>In the <a href="../figure_3-6.png">Figure 3-6</a> example (above), we are looking for the key 251, so we know that we need to follow the page reference between the boundaries 200 and 300. That takes us to a similar-looking page that further breaks down the 200–300 range into subranges. Eventually we get down to a page containing individual keys (a <em>leaf page</em>), which either contains the value for each key inline or contains references to the pages where the values can be found.</p>
<p>The number of references to child pages in one page of the B-tree is called the <a href="https://en.wikipedia.org/wiki/Branching_factor"><em>branching factor</em></a>. For example, in Figure 3-6 the branching factor is six. In practice, the branching factor depends on the amount of space required to store the page references and the range boundaries, but typically it is several hundred.</p>
<ul>
<li>If you want to update the value for an existing key in a B-tree, you search for the leaf page containing that key, change the value in that page, and write the page back to disk (any references to that page remain valid).</li>
<li>If you want to add a new key, you need to find the page whose range encompasses the new key and add it to that page. If there isn't enough free space in the page to accommodate the new key, it is split into two half-full pages, and the parent page is updated to account for the new subdivision of key ranges. See <a href="../figure_3-7.png">Figure 3-7</a> (below).</li>
</ul>
<p><a href="../figure_3-7.png" title="Figure 3-7. Growing a B-tree by splitting a page."><img alt="Figure 3-7. Growing a B-tree by splitting a page." src="../figure_3-7_600.png" /></a></p>
<p>This algorithm ensures that the tree remains <em>balanced</em>: a B-tree with n keys always has a depth of <em>O(log n)</em>. Most databases can fit into a B-tree with a depth of three or four levels, so you don't need to follow many page references to find the page. (A four-level tree of 4 KB pages with a branching factor of 500 can store up to 256 TB.)</p>
<h5 id="making-b-trees-reliable"><strong>Making B-trees reliable</strong><a class="headerlink" href="#making-b-trees-reliable" title="Permanent link">&para;</a></h5>
<p>The underlying write operation of a B-tree is to overwrite a page on disk with new data. It is assumed that the overwrite does not change the location of the page: all references to that page remain intact when the page is overwritten. This is in contrast to log-structured indexes such as LSM-trees, which only append to files (and eventually delete obsolete files) but never modify files in place.</p>
<p>You can think of overwriting a page on disk as an actual hardware operation:</p>
<ul>
<li>On a magnetic hard drive, this means moving the disk head to the right place, waiting for the right position on the spinning platter to come around, and then overwriting the appropriate sector with new data.</li>
<li>On SSDs, what happens is somewhat more complicated, due to the fact that an SSD must erase and rewrite fairly large blocks of a storage chip at a time.</li>
</ul>
<p>Moreover, some operations require several different pages to be overwritten. For example, if you split a page because an insertion caused it to be overfull, you need to write the two pages that were split, and also overwrite their parent page to update the references to the two child pages. This is a dangerous operation, because if the database crashes after only some of the pages have been written, you end up with a corrupted index (e.g., there may be an <em>orphan</em> page that is not a child of any parent).</p>
<p>In order to make the database resilient to crashes, it is common for B-tree implementations to include an additional data structure on disk: a <a href="https://en.wikipedia.org/wiki/Write-ahead_logging"><em>write-ahead log</em></a> (WAL, also known as a <a href="https://en.wikipedia.org/wiki/Redo_log"><em>redo log</em></a>). This is an append-only file to which every B-tree modification must be written before it can be applied to the pages of the tree itself. When the database comes back up after a crash, this log is used to restore the B-tree back to a consistent state.</p>
<p>Concurrency control is required if multiple threads are going to access the B-tree at the same time, otherwise a thread may see the tree in an inconsistent state. This is typically done by protecting the tree's data structures with <em>latches</em> (lightweight locks). (In this regard, log-structured approaches are simpler in this regard, because they do all the merging in the background without interfering with incoming queries and atomically swap old segments for new segments from time to time.)</p>
<h5 id="b-tree-optimizations"><strong>B-tree optimizations</strong><a class="headerlink" href="#b-tree-optimizations" title="Permanent link">&para;</a></h5>
<p>Many optimizations for B-trees have been developed over the years, such as:</p>
<ul>
<li>Instead of overwriting pages and maintaining a write-ahead log for crash recovery, some databases (like <a href="https://en.wikipedia.org/wiki/Lightning_Memory-Mapped_Database">LMDB</a>) use a <a href="https://en.wikipedia.org/wiki/Copy-on-write">copy-on-write</a> scheme. A modified page is written to a different location, and a new version of the parent pages in the tree is created, pointing at the new location. This approach is also useful for concurrency control (see <a href="ch7.md#snapshot-isolation-and-repeatable-read">Snapshot Isolation and Repeatable Read</a> in Chapter 7).</li>
<li>We can save space in pages by not storing the entire key, but abbreviating it. Especially in pages on the interior of the tree, keys only need to provide enough information to act as boundaries between key ranges. Packing more keys into a page allows the tree to have a higher branching factor, and thus fewer levels. (This variant is sometimes known as a <a href="https://en.wikipedia.org/wiki/B%2B_tree">B+ tree</a>, although the optimization is so common that it often isn't distinguished from other B-tree variants.)</li>
<li>In general, pages can be positioned anywhere on disk; there is nothing requiring pages with nearby key ranges to be nearby on disk. If a query needs to scan over a large part of the key range in sorted order, that page-by-page layout can be inefficient, because a disk seek may be required for every page that is read. Many B-tree implementations therefore try to lay out the tree so that leaf pages appear in sequential order on disk. However, it's difficult to maintain that order as the tree grows.<ul>
<li>By contrast, since LSM-trees rewrite large segments of the storage in one go during merging, it's easier for them to keep sequential keys close to each other on disk.</li>
</ul>
</li>
<li>Additional pointers have been added to the tree. For example, each leaf page may have references to its sibling pages to the left and right, which allows scanning keys in order without jumping back to parent pages.</li>
<li>B-tree variants such as <a href="https://en.wikipedia.org/wiki/Fractal_tree_index"><em>fractal trees</em></a> borrow some log-structured ideas to reduce disk seeks.</li>
</ul>
<h4 id="comparing-b-trees-and-lsm-trees">Comparing B-Trees and LSM-Trees<a class="headerlink" href="#comparing-b-trees-and-lsm-trees" title="Permanent link">&para;</a></h4>
<p>B-tree implementations are generally more mature, but LSM-trees are also interesting due to their performance characteristics.</p>
<p>As a rule of thumb, <u>LSM-trees are typically faster for writes, whereas B-trees are thought to be faster for reads. Reads are typically slower on LSM-trees because they have to check several different data structures and SSTables at different stages of compaction.</u></p>
<p>However, benchmarks are often inconclusive and sensitive to details of the workload.  You need to test systems with your particular workload in order to make a valid comparison.</p>
<h5 id="advantages-of-lsm-trees"><strong>Advantages of LSM-trees</strong><a class="headerlink" href="#advantages-of-lsm-trees" title="Permanent link">&para;</a></h5>
<p>A B-tree index must write every piece of data at least twice: once to the write-ahead log, and once to the tree page itself (and again as pages are split). There is also overhead from having to write an entire page at a time, even if only a few bytes in that page changed. Some storage engines even overwrite the same page twice in order to avoid ending up with a partially updated page in the event of a power failure (such as <a href="https://www.percona.com/blog/2006/08/04/innodb-double-write/">InnoDB</a>).</p>
<p>Log-structured indexes also rewrite data multiple times due to repeated compaction and merging of SSTables. One write to the database resulting in multiple writes to the disk over the course of the database's lifetime. This effect is known as <a href="https://en.wikipedia.org/wiki/Write_amplification"><em>write amplification</em></a>. It is of particular concern on SSDs, which can only overwrite blocks a limited number of times before wearing out (see also <a href="https://en.wikipedia.org/wiki/Wear_leveling">wear leveling</a>).</p>
<p>In write-heavy applications, the performance bottleneck might be the rate at which the database can write to disk. In this case, write amplification has a direct performance cost: the more that a storage engine writes to disk, the fewer writes per second it can handle within the available disk bandwidth.</p>
<p>Moreover, LSM-trees are typically able to sustain higher write throughput than B-trees, partly because:</p>
<ul>
<li>They sometimes have lower write amplification (although this depends on the storage engine configuration and workload),</li>
<li>They sequentially write compact SSTable files rather than having to overwrite several pages in the tree.</li>
</ul>
<p>This difference is particularly important on magnetic hard drives, where sequential writes are much faster than random writes.</p>
<p>LSM-trees can be compressed better, and thus often produce smaller files on disk than B-trees. B-tree storage engines leave some disk space unused due to fragmentation: when a page is split or when a row cannot fit into an existing page, some space in a page remains unused. Since LSM-trees are not page-oriented and periodically rewrite SSTables to remove fragmentation, they have lower storage overheads, especially when using leveled compaction</p>
<p>On many SSDs, the firmware internally uses a log-structured algorithm to turn random writes into sequential writes on the underlying storage chips, so the impact of the storage engine's write pattern is less pronounced. However, lower write amplification and reduced fragmentation are still advantageous on SSDs: representing data more compactly allows more read and write requests within the available I/O bandwidth.</p>
<h5 id="downsides-of-lsm-trees"><strong>Downsides of LSM-trees</strong><a class="headerlink" href="#downsides-of-lsm-trees" title="Permanent link">&para;</a></h5>
<p>A downside of log-structured storage is that the compaction process can sometimes interfere with the performance of ongoing reads and writes. Since disks have limited resources, so it can easily happen that a request needs to wait while the disk finishes an expensive compaction operation. The impact on throughput and average response time is usually small, but at higher percentiles the response time of queries to log-structured storage engines can sometimes be quite high, and B-trees can be more predictable.</p>
<p>At high write throughput, the disk's finite write bandwidth needs to be shared between the initial write (logging and flushing a memtable to disk) and the compaction threads running in the background. The bigger the database gets, the more disk bandwidth is required for compaction.</p>
<p>If write throughput is high and compaction is not configured carefully, it can happen that compaction cannot keep up with the rate of incoming writes. In this case, the number of unmerged segments on disk keeps growing until you run out of disk space, and reads also slow down because they need to check more segment files. Typically, SSTable-based storage engines do not throttle the rate of incoming writes, even if compaction cannot keep up, so you need explicit monitoring to detect this situation.</p>
<h5 id="advantages-of-b-trees"><strong>Advantages of B-trees</strong> *<a class="headerlink" href="#advantages-of-b-trees" title="Permanent link">&para;</a></h5>
<p>An advantage of B-trees is that each key exists in exactly one place in the index, whereas a log-structured storage engine may have multiple copies of the same key in different segments. This aspect makes B-trees attractive in databases that want to offer strong transactional semantics: in many relational databases, transaction isolation is implemented using locks on ranges of keys, and in a B-tree index, those locks can be directly attached to the tree. See more details in <a href="ch7.md">Chapter 7</a>.</p>
<p>B-trees are very ingrained in the architecture of databases and provide consistently good performance for many workloads, so it's unlikely that they will go away anytime soon. In new datastores, log-structured indexes are becoming increasingly popular. There is no quick and easy rule for determining which type of storage engine is better for your use case, so it is worth testing empirically.</p>
<h4 id="other-indexing-structures">Other Indexing Structures<a class="headerlink" href="#other-indexing-structures" title="Permanent link">&para;</a></h4>
<p>So far we have only discussed key-value indexes, which are like a <a href="https://en.wikipedia.org/wiki/Primary_key"><em>primary key</em></a> index in the relational model. A primary key uniquely identifies one row in a relational table, or one document in a document database, or one vertex in a graph database. Other records in the database can refer to that row/document/vertex by its primary key (or ID), and the index is used to resolve such references.</p>
<p>It is also very common to have <em>secondary indexes</em>. In relational databases, you can create several secondary indexes on the same table using the CREATE INDEX command, and they are often crucial for performing joins efficiently. For example, in <a href="../figure_2-1.png">Figure 2-1</a> in <a href="../ch2/">Chapter 2</a> you would most likely have a secondary index on the <code>user_id</code> columns so that you can find all the rows belonging to the same user in each of the tables.</p>
<p>A secondary index can easily be constructed from a key-value index. The main difference is that the indexed values in a secondary index are not necessarily unique; that is, there might be many rows (documents, vertices) under the same index entry. This can be solved in two ways: either by making each value in the index a list of matching row identifiers (like a postings list in a full-text index) or by making each entry unique by appending a row identifier to it. Either way, both B-trees and log-structured indexes can be used as secondary indexes.</p>
<h5 id="storing-values-within-the-index"><strong>Storing values within the index</strong><a class="headerlink" href="#storing-values-within-the-index" title="Permanent link">&para;</a></h5>
<p>The key in an index is the thing that queries search for, but the value can be one of two things: it could be the actual row (document, vertex), or it could be a reference to the row. In the latter case, the place where rows are stored is known as a <em>heap file</em>, and it stores data in no particular order (it may be append-only, or it may keep track of deleted rows in order to overwrite them with new data later). The heap file approach is common because it avoids duplicating data when multiple secondary indexes are present: each index just references a location in the heap file, and the actual data is kept in one place.</p>
<p>When updating a value without changing the key, the heap file approach can be quite efficient: the record can be overwritten in place, provided that the new value is not larger than the old value. The situation is more complicated if the new value is larger, as it probably needs to be moved to a new location in the heap where there is enough space. In that case, either all indexes need to be updated to point at the new heap location of the record, or a forwarding pointer is left behind in the old heap location.</p>
<p>In some situations, the extra hop from the index to the heap file is too much of a performance penalty for reads, so it can be desirable to store the indexed row directly within an index. This is known as a <em>clustered index</em>. For example, in MySQL's InnoDB storage engine, the primary key of a table is always a clustered index, and secondary indexes refer to the primary key (rather than a heap file location). In SQL Server, you can specify one clustered index per table.</p>
<p>A compromise between a clustered index (storing all row data within the index) and a nonclustered index (storing only references to the data within the index) is known as a <a href="https://en.wikipedia.org/wiki/Database_index#Covering_index"><em>covering index</em></a> or <em>index with included columns</em>, which stores some of a table's columns within the index. This allows some queries to be answered by using the index alone (in which case, the index is said to cover the query).</p>
<p>As with any kind of duplication of data, clustered and covering indexes can speed up reads, but they require additional storage and can add overhead on writes. Databases also need to go to additional effort to enforce transactional guarantees, because applications should not see inconsistencies due to the duplication.</p>
            </div>
        </div>

        <footer class="col-md-12">
            
        </footer>

        <script src="../../js/jquery-1.10.2.min.js"></script>
        <script src="../../js/bootstrap-3.0.3.min.js"></script>
        <script src="../../js/highlight.pack.js"></script>
        <script src="../../js/base.js"></script>
        <script src="../../custom.js"></script>
    </body>
</html>