<!DOCTYPE html>
<html lang="en">
    <head>
        <!-- Global Site Tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-59055167-2"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments)};
          gtag('js', new Date());

          gtag('config', 'UA-59055167-2');
        </script>

        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        <link rel="canonical" href="https://notes.shichao.io/bd/ch1/">
        <link rel="shortcut icon" href="../../toki_32.png">
        

	<title>Chapter 1. A new paradigm for Big Data - Shichao's Notes</title>

        <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../css/font-awesome-4.0.3.css" rel="stylesheet">
        <link rel="stylesheet" href="../../css/highlight.css">
        <link href="../../css/base.css" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,400italic,500,600" rel="stylesheet">
        <link href="../../custom.css" rel="stylesheet">
        <link href="../../friendly.css" rel="stylesheet">
        <link href="../../theme.css" rel="stylesheet">

        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        
    </head>

    <body>

        
<div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <!-- Main title -->
            <a class="navbar-brand" href="../..">Shichao's Notes</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
            <!-- Main navigation -->
            <ul class="nav navbar-nav">
            <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">APUE <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        <li>
                            <a href="../../apue/">Contents</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch1/">Chapter 1. UNIX System Overview</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch2/">Chapter 2. UNIX Standardization and Implementations</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch3/">Chapter 3. File I/O</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch4/">Chapter 4. Files and Directories</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch5/">Chapter 5. Standard I/O Library</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch6/">Chapter 6. System Data Files and Information</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch7/">Chapter 7. Process Environment</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch8/">Chapter 8. Process Control</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch9/">Chapter 9. Process Relationships</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch10/">Chapter 10. Signals</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch11/">Chapter 11. Threads</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch12/">Chapter 12. Thread Control</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch13/">Chapter 13. Daemon Processes</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch14/">Chapter 14. Advanced I/O</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch15/">Chapter 15. Interprocess Communication</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch16/">Chapter 16. Network IPC: Sockets</a>
                        </li>
                      
                        <li>
                            <a href="../../apue/ch17/">Chapter 17. Advanced IPC</a>
                        </li>
                      
                    </ul>
                </li>
            <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">UNP <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        <li>
                            <a href="../../unp/">Contents</a>
                        </li>
                      
                        <li>
                            <a href="../../unp/ch1/">Chapter 1. Introduction</a>
                        </li>
                      
                        <li>
                            <a href="../../unp/ch2/">Chapter 2. The Transport Layer: TCP, UDP, and SCTP</a>
                        </li>
                      
                        <li>
                            <a href="../../unp/ch3/">Chapter 3. Sockets Introduction</a>
                        </li>
                      
                        <li>
                            <a href="../../unp/ch4/">Chapter 4. Elementary TCP Sockets</a>
                        </li>
                      
                        <li>
                            <a href="../../unp/ch5/">Chapter 5. TCP Client/Server Example</a>
                        </li>
                      
                        <li>
                            <a href="../../unp/ch6/">Chapter 6. I/O Multiplexing: The select and poll Functions</a>
                        </li>
                      
                        <li>
                            <a href="../../unp/ch7/">Chapter 7. Socket Options</a>
                        </li>
                      
                        <li>
                            <a href="../../unp/ch8/">Chapter 8. Elementary UDP Sockets</a>
                        </li>
                      
                    </ul>
                </li>
            <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">TCPv1 <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        <li>
                            <a href="../../tcpv1/">Contents</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch1/">Chapter 1. Introduction</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch2/">Chapter 2. The Internet Address Architecture</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch3/">Chapter 3. Link Layer</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch4/">Chapter 4. ARP: Address Resolution Protocol</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch5/">Chapter 5. The Internet Protocol (IP)</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch6/">Chapter 6. System Configuration: DHCP and Autoconfiguration</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch7/">Chapter 7. Firewalls and Network Address Translation (NAT)</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch8/">Chapter 8. ICMPv4 and ICMPv6: Internet Control Message Protocol</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch9/">Chapter 9. Broadcasting and Local Multicasting (IGMP and MLD)</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch10/">Chapter 10. User Datagram Protocol (UDP) and IP Fragmentation</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch11/">Chapter 11. Name Resolution and the Domain Name System (DNS)</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch12/">Chapter 12. TCP: The Transmission Control Protocol (Preliminaries)</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch13/">Chapter 13. TCP Connection Management</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch14/">Chapter 14. TCP Timeout and Retransmission</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch15/">Chapter 15. TCP Data Flow and Window Management</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch16/">Chapter 16. TCP Congestion Control</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch17/">Chapter 17. TCP Keepalive</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/ch18/">Chapter 18. Security: EAP, IPsec, TLS, DNSSEC, and DKIM</a>
                        </li>
                      
                        <li>
                            <a href="../../tcpv1/headers/">Headers</a>
                        </li>
                      
                    </ul>
                </li>
            <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">GOPL <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        <li>
                            <a href="../../gopl/">Contents</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch1/">Chapter 1. Tutorial</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch2/">Chapter 2. Program Structure</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch3/">Chapter 3. Basic Data Types</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch4/">Chapter 4. Composite Types</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch5/">Chapter 5. Functions</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch6/">Chapter 6. Methods</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch7/">Chapter 7. Interfaces</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch8/">Chapter 8. Goroutines and Channels</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch9/">Chapter 9. Concurrency with Shared Variables</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch10/">Chapter 10. Packages and the Go Tool</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch11/">Chapter 11. Testing</a>
                        </li>
                      
                        <li>
                            <a href="../../gopl/ch12/">Chapter 12. Reflection</a>
                        </li>
                      
                    </ul>
                </li>
            <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">CSN <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        <li>
                            <a href="../../csn/">Contents</a>
                        </li>
                      
                        <li>
                            <a href="../../csn/part1/">Part 1: Language</a>
                        </li>
                      
                        <li>
                            <a href="../../csn/part2/">Part 2: Advanced</a>
                        </li>
                      
                    </ul>
                </li>
            <li>
                    <a href="../../toc/">TOC</a>
                </li>
            </ul>
            <!-- Search, Navigation and Repo links -->
            <ul class="nav navbar-nav navbar-right">
                
                <li>
                    
                        <a href="https://github.com/shichao-an/notes/blob/master/docs/bd/ch1.md">
                    
                        
                            <i class="fa fa-github"></i>
                        
                        GitHub
                    </a>
                </li>
                
            </ul>
        </div>
    </div>
</div>

        <div class="container">
            <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
    
        <li class="main active"><a href="#chapter-1-a-new-paradigm-for-big-data">Chapter 1. A new paradigm for Big Data</a></li>
        
    
        <li class="main "><a href="#how-this-book-is-structured">How this book is structured</a></li>
        
    
        <li class="main "><a href="#scaling-with-a-traditional-database">Scaling with a traditional database</a></li>
        
            <li><a href="#scaling-with-a-queue">Scaling with a queue</a></li>
        
            <li><a href="#scaling-by-sharding-the-database">Scaling by sharding the database</a></li>
        
            <li><a href="#fault-tolerance-issues-begin">Fault-tolerance issues begin</a></li>
        
            <li><a href="#corruption-issues">Corruption issues</a></li>
        
            <li><a href="#what-went-wrong">What went wrong?</a></li>
        
    
        <li class="main "><a href="#how-will-big-data-techniques-help">How will Big Data techniques help?</a></li>
        
    
        <li class="main "><a href="#nosql-is-not-a-panacea">NoSQL is not a panacea</a></li>
        
    
        <li class="main "><a href="#first-principles">First principles</a></li>
        
    
        <li class="main "><a href="#desired-properties-of-a-big-data-system">Desired properties of a Big Data system</a></li>
        
            <li><a href="#robustness-and-fault-tolerance">Robustness and fault tolerance</a></li>
        
            <li><a href="#low-latency-reads-and-updates">Low latency reads and updates</a></li>
        
            <li><a href="#scalability">Scalability</a></li>
        
            <li><a href="#generalization">Generalization</a></li>
        
            <li><a href="#extensibility">Extensibility</a></li>
        
            <li><a href="#ad-hoc-queries">Ad hoc queries</a></li>
        
            <li><a href="#minimal-maintenance">Minimal maintenance</a></li>
        
            <li><a href="#debuggability">Debuggability</a></li>
        
    
        <li class="main "><a href="#the-problems-with-fully-incremental-architectures">The problems with fully incremental architectures</a></li>
        
            <li><a href="#operational-complexity">Operational complexity</a></li>
        
            <li><a href="#extreme-complexity-of-achieving-eventual-consistency">Extreme complexity of achieving eventual consistency</a></li>
        
            <li><a href="#lack-of-human-fault-tolerance">Lack of human-fault tolerance</a></li>
        
            <li><a href="#fully-incremental-solution-vs-lambda-architecture-solution">Fully incremental solution vs. Lambda Architecture solution</a></li>
        
    
        <li class="main "><a href="#lambda-architecture">Lambda Architecture</a></li>
        
            <li><a href="#batch-layer">Batch layer</a></li>
        
            <li><a href="#serving-layer">Serving layer</a></li>
        
            <li><a href="#batch-and-serving-layers-satisfy-almost-all-properties">Batch and serving layers satisfy almost all properties</a></li>
        
    
        <li class="main "><a href="#speed-layer">Speed layer</a></li>
        
            <li><a href="#example-of-the-web-analytics-application-continued">Example of the web analytics application (continued) *</a></li>
        
            <li><a href="#eventual-accuracy">Eventual accuracy *</a></li>
        
            <li><a href="#performance-and-robustness">Performance and robustness *</a></li>
        
    
        <li class="main "><a href="#recent-trends-in-technology">Recent trends in technology</a></li>
        
            <li><a href="#cpus-arent-getting-faster">CPUs aren't getting faster</a></li>
        
            <li><a href="#elastic-clouds">Elastic clouds</a></li>
        
            <li><a href="#vibrant-open-source-ecosystem-for-big-data">Vibrant open source ecosystem for Big Data</a></li>
        
    
        <li class="main "><a href="#example-application-superwebanalyticscom">Example application: SuperWebAnalytics.com</a></li>
        
    
        <li class="main "><a href="#summary">Summary</a></li>
        
    
        <li class="main "><a href="#doubts-and-solutions">Doubts and Solutions</a></li>
        
            <li><a href="#verbatim">Verbatim</a></li>
        
    
    </ul>
</div></div>
            <div class="col-md-9" role="main">
              

<h3 id="chapter-1-a-new-paradigm-for-big-data"><strong>Chapter 1. A new paradigm for Big Data</strong><a class="headerlink" href="#chapter-1-a-new-paradigm-for-big-data" title="Permanent link">&para;</a></h3>
<p>[p1-2]</p>
<p>Traditional systems, and the data management techniques associated with them, have failed to scale to Big Data.</p>
<p>To tackle the challenges of Big Data, a lot of new technologies have emerged, many of which have been grouped under the term <em>NoSQL</em>. In some ways, these new technologies are more complex than traditional databases, and in other ways they're simpler. These systems can scale to vastly larger sets of data, but using these technologies effectively requires a fundamentally new set of techniques. They aren't one-size-fits-all solutions.</p>
<p>Many of these Big Data systems were pioneered by Google, including:</p>
<ul>
<li>Distributed filesystems,</li>
<li>The  MapReduce computation framework,</li>
<li>Distributed locking services.</li>
</ul>
<p>Another notable pioneer in the space was Amazon, which created an innovative distributed key/value store called Dynamo. The open source community responded in the years following with Hadoop, HBase, MongoDB, Cassandra, RabbitMQ, and countless other projects.</p>
<p>This book is about complexity as much as it is about scalability. Some of the most basic ways people manage data in traditional systems like relational database management systems (RDBMSs) are too complex for Big Data systems.The simpler, alternative approach is the new paradigm for Big Data. This approach is dubbed the <a href="https://en.wikipedia.org/wiki/Lambda_architecture"><strong>Lambda Architecture</strong></a>.</p>
<h3 id="how-this-book-is-structured">How this book is structured<a class="headerlink" href="#how-this-book-is-structured" title="Permanent link">&para;</a></h3>
<p>This book is a theory book, focusing on how to approach building a solution to any Big Data problem. It is structured into theory and illustration chapters.</p>
<h3 id="scaling-with-a-traditional-database">Scaling with a traditional database<a class="headerlink" href="#scaling-with-a-traditional-database" title="Permanent link">&para;</a></h3>
<p>The example in this section is a simple web analytics application, which tracks the number of pageviews for any URL a customer wishes to track. The customer's web page pings the application's web server with its URL every time a pageview is received. Additionally, the application should be able to tell you at any point what the top 100 URLs are by number of pageviews.</p>
<p>You start with a traditional relational schema for the pageviews similiar to the table below:</p>
<table>
<thead>
<tr>
<th>Column name</th>
<th>Type</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>id</code></td>
<td><code>integer</code></td>
</tr>
<tr>
<td><code>user_id</code></td>
<td><code>integer</code></td>
</tr>
<tr>
<td><code>url</code></td>
<td><code>varchar(255)</code></td>
</tr>
<tr>
<td><code>pageviews</code></td>
<td><code>bigint</code></td>
</tr>
</tbody>
</table>
<p>Your back end consists of an RDBMS with a table of that schema and a web server. Whenever someone loads a web page being tracked by your application, the web page pings your web server with the pageview, and your web server increments the corresponding row in the database.</p>
<p>The following subsections discuss what problems emerge as you evolve the application: you'll run into problems with both scalability and complexity.</p>
<h4 id="scaling-with-a-queue">Scaling with a queue<a class="headerlink" href="#scaling-with-a-queue" title="Permanent link">&para;</a></h4>
<p>As the traffic to your application is growing, you got a lot of "Timeout error on inserting to the database" error, since the database can't keep up with the load, so write requests to increment pageviews are timing out.</p>
<p>Instead of having the web server hit the database directly, you insert a queue between the web server and the database. Whenever you receive a new pageview, that event is added to the queue. You then create a worker process that reads 100 events at a time off the queue, and batches them into a single database update. This is illustrated in the figure below:</p>
<p><a href="../figure_1.2.png" title="Figure 1.2 Batching updates with queue and worker"><img alt="Figure 1.2 Batching updates with queue and worker" src="../figure_1.2.png" /></a></p>
<p>This scheme resolves the timeout issues you were getting. If the database ever gets overloaded again, the queue will just get bigger instead of timing out to the web server and potentially losing data.</p>
<h4 id="scaling-by-sharding-the-database">Scaling by sharding the database<a class="headerlink" href="#scaling-by-sharding-the-database" title="Permanent link">&para;</a></h4>
<p>As your application continues to get more and more popular, and again the database gets overloaded. Your worker can't keep up with the writes; adding more workers to parallelize the updates doesn't help; the database is clearly the bottleneck.</p>
<p>The approach is to use multiple database servers and spread the table across all the servers. Each server will have a subset of the data for the table. This is known as <a href="https://en.wikipedia.org/wiki/Partition_(database)"><strong>horizontal partitioning</strong></a> or <a href="https://en.wikipedia.org/wiki/Shard_(database_architecture)"><strong>sharding</strong></a>. This technique spreads the write load across multiple machines.</p>
<p>The sharding technique you use is to choose the shard for each key by taking the hash of the key modded by the number of shards. Mapping keys to shards using a hash function causes the keys to be uniformly distributed across the shards. You do the following:</p>
<ol>
<li>Write a script to map over all the rows in your single database instance, and split the data into four shards. Since it takes a while to run this script, you turn off the worker that increments pageviews to avoid losing increments during the transition.</li>
<li>Wrap a library around database-handling code that reads the number of shards from a configuration file, and redeploy all of your application code, since all application code needs to know how to find the shard for each key. You have to modify your top-100-URLs query to get the top 100 URLs from each shard and merge those together for the global top 100 URLs.</li>
</ol>
<p>As the application gets more popular, you keep having to reshard the database into more shards to keep up with the write load:</p>
<ul>
<li>Each time gets more and more painful because there's so much more work to coordinate. You can't just run one script to do the resharding, as that would be too slow. You have to do all the resharding in parallel and manage many active worker scripts at once.</li>
<li>If you forget to update the application code with the new number of shards, it causes many of the increments to be written to the wrong shards. So you have to write a one-off script to manually go through the data and move whatever was misplaced.</li>
</ul>
<h4 id="fault-tolerance-issues-begin">Fault-tolerance issues begin<a class="headerlink" href="#fault-tolerance-issues-begin" title="Permanent link">&para;</a></h4>
<p>With so many shards, it becomes a frequent occurrence for the disk on one of the database machines to go bad. That portion of the data is unavailable while that machine is down. You do a couple of things to address this:</p>
<ul>
<li>You update your queue/worker system to put increments for unavailable shards on a separate “pending” queue that you attempt to flush once every five minutes.</li>
<li>You use the database's replication capabilities to add a slave to each shard so you have a backup in case the master goes down. You don't write to the slave, but at least customers can still view the stats in the application.</li>
</ul>
<h4 id="corruption-issues">Corruption issues<a class="headerlink" href="#corruption-issues" title="Permanent link">&para;</a></h4>
<p>You accidentally deploy a bug to production that increments the number of pageviews by two, instead of by one, for every URL and you don't notice until 24 hours later, but by then the damage is done. Your weekly backups don't help because there's no way of knowing which data got corrupted.  After all this work trying to make your system scalable and tolerant of machine failures, your system has no resilience to a human making a mistake.</p>
<h4 id="what-went-wrong">What went wrong?<a class="headerlink" href="#what-went-wrong" title="Permanent link">&para;</a></h4>
<p>As the application evolved, the system continued to get more and more complex: queues, shards, replicas, resharding scripts, etc. Developing applications on the data requires a lot more than just knowing the database schema; your code needs to know how to talk to the right shards, and if you make a mistake, there's nothing preventing you from reading from or writing to the wrong shard.</p>
<p>One problem is that your database is not self-aware of its distributed nature, so it can't help you deal with shards, replication, and distributed queries. All that complexity got pushed to you both in operating the database and developing the application code.</p>
<p>However, the worst problem is that the system is not engineered for human mistakes.  As the system keeps getting more complex, it is more likely that a mistake will be made:</p>
<ul>
<li>Mistakes in software are inevitable. If you're not engineering for it, you might as well be writing scripts that randomly corrupt data.</li>
<li>Backups are not enough; the system must be carefully thought out to limit the damage a human mistake can cause.</li>
<li>Human-fault tolerance is not optional. It's essential, especially when Big Data adds so many more complexities to building applications.</li>
</ul>
<h3 id="how-will-big-data-techniques-help">How will Big Data techniques help?<a class="headerlink" href="#how-will-big-data-techniques-help" title="Permanent link">&para;</a></h3>
<p>The Big Data techniques to be discussed address these scalability and complexity issues in dramatically:</p>
<ol>
<li>The databases and computation systems for Big Data are aware of their distributed nature. Sharding and replication are handled for you.<ul>
<li>Shading: the logic is internalized in the database, preventing situations where you accidentally query the wrong shard.</li>
<li>Scaling: just add new nodes and the systems will automatically rebalance onto the new nodes.</li>
</ul>
</li>
<li>Make data immutable. Instead of storing the pageview counts as your core dataset, which you continuously mutate as new pageviews come in, you store the raw pageview information, which is never modified. <u>When you make a mistake, you might write bad data, but at least you won't destroy good data.</u> This is a much stronger human-fault tolerance guarantee than in a traditional system based on mutation. [p6]</li>
</ol>
<h3 id="nosql-is-not-a-panacea">NoSQL is not a panacea<a class="headerlink" href="#nosql-is-not-a-panacea" title="Permanent link">&para;</a></h3>
<p>Innovation in scalable data systems in the past decades include:</p>
<ul>
<li>Large-scale computation systems: such as <a href="https://en.wikipedia.org/wiki/Apache_Hadoop">Hadoop</a></li>
<li>Databases: such as <a href="https://en.wikipedia.org/wiki/Apache_Cassandra">Cassandra</a> and <a href="https://en.wikipedia.org/wiki/Riak">Riak</a>.</li>
</ul>
<p>These systems can handle very large amounts of data, but with serious trade-offs:</p>
<ul>
<li>Hadoop can parallelize large-scale batch computations on very large amounts of data, but the computations have high latency. You don't use Hadoop for anything where you need low-latency results.</li>
<li>NoSQL databases like Cassandra achieve their scalability by offering you a much more limited data model than you're used to with something like SQL.<ul>
<li>Squeezing your application into these limited data models can be very complex.</li>
<li>They are not human-fault tolerant, because the databases are mutable.</li>
</ul>
</li>
</ul>
<p>These tools on their own are not a panacea. But when intelligently used in conjunction with one another, you can produce scalable systems for arbitrary data problems with human-fault tolerance and a minimum of complexity. This is the Lambda Architecture discussed throughout the book.</p>
<h3 id="first-principles">First principles<a class="headerlink" href="#first-principles" title="Permanent link">&para;</a></h3>
<p>What does a data system do? An intuitive definition is:</p>
<blockquote>
<p>A data system answers questions based on information that was acquired in the past up to the present.</p>
</blockquote>
<ul>
<li>Data systems don't just memorize and regurgitate information. They combine bits and pieces together to produce their answers.</li>
<li>All bits of information are equal. Some information is derived from other pieces of information.</li>
<li>When you keep tracing back where information is derived from, you eventually end up at information that's not derived from anything. This is the rawest information you have: information you hold to be true simply because it exists. This information is called <em>data</em>.</li>
</ul>
<p>Data is often used interchangeably with the word <em>information</em>. But for the remainder of this book, when we use the word data, we're referring to that special information from which everything else is derived.</p>
<p>The most general-purpose data system answers questions by looking at the entire dataset, which has the definition:</p>
<blockquote>
<p>query = function(all data)</p>
</blockquote>
<p>[p7]</p>
<p>The Lambda Architecture provides a general-purpose approach to implementing an arbitrary function on an arbitrary dataset and having the function return its results with low latency. This does not mean always using the same technologies to implement a database system; the Lambda Architecture defines a consistent approach to choosing those technologies and to wiring them together to meet your requirements.</p>
<h3 id="desired-properties-of-a-big-data-system">Desired properties of a Big Data system<a class="headerlink" href="#desired-properties-of-a-big-data-system" title="Permanent link">&para;</a></h3>
<p>Not only must a Big Data system perform well and be resource-efficient, it must be easy to reason about as well.</p>
<h4 id="robustness-and-fault-tolerance">Robustness and fault tolerance<a class="headerlink" href="#robustness-and-fault-tolerance" title="Permanent link">&para;</a></h4>
<p>Systems need to behave correctly despite any of the following situations:</p>
<ul>
<li>Machines going down randomly</li>
<li>The complex semantics of consistency in distributed databases</li>
<li>Duplicated data</li>
<li>Concurrency</li>
</ul>
<p>These challenges make it difficult even to reason about a system is doing. Part of making a Big Data system robust is avoiding these complexities so that you can easily reason about the system</p>
<p>It's imperative for systems to be <em>human-fault tolerant</em>, which is an oft-overlooked property. In a production system, it's inevitable that someone will make a mistake, such as by deploying incorrect code that corrupts values in a database. If you build immutability and recomputation into the core of a Big Data system, the system will be innately resilient to human error by providing a clear and simple mechanism for recovery.</p>
<h4 id="low-latency-reads-and-updates">Low latency reads and updates<a class="headerlink" href="#low-latency-reads-and-updates" title="Permanent link">&para;</a></h4>
<ul>
<li>Most applications require reads to be satisfied with very low latency, typically
between a few milliseconds to a few hundred milliseconds.</li>
<li>The update latency requirements vary a great deal between applications. Some applications require updates to propagate immediately, but in other applications a latency of a few hours is fine.</li>
</ul>
<p>You need to be able to:</p>
<ul>
<li>Achieve low latency updates when you need them in your Big Data systems,</li>
<li>Achieve low latency reads and updates without compromising the robustness of the system.</li>
</ul>
<h4 id="scalability">Scalability<a class="headerlink" href="#scalability" title="Permanent link">&para;</a></h4>
<p><u>Scalability is the ability to maintain performance in the face of increasing data or load by adding resources to the system.</u> The Lambda Architecture is horizontally scalable across all layers of the system stack: scaling is accomplished by adding more machines.</p>
<h4 id="generalization">Generalization<a class="headerlink" href="#generalization" title="Permanent link">&para;</a></h4>
<p>A general system can support a wide range of applications. Because the Lambda Architecture is based on functions of all data, it generalizes to all applications.</p>
<h4 id="extensibility">Extensibility<a class="headerlink" href="#extensibility" title="Permanent link">&para;</a></h4>
<p>YExtensible systems allow functionality to be added with a minimal development cost, without having to reinvent the wheel each time you add a related feature or make a change to how your system works.</p>
<p>Oftentimes a new feature or a change to an existing feature requires a migration of old data into a new format. Part of making a system extensible is making it easy to do large-scale migrations. Being able to do big migrations quickly and easily is core to the approach under discussion.</p>
<h4 id="ad-hoc-queries">Ad hoc queries<a class="headerlink" href="#ad-hoc-queries" title="Permanent link">&para;</a></h4>
<p>Every large dataset has unanticipated value within it. Being able to mine a dataset arbitrarily gives opportunities for business optimization and new applications. Ultimately, you can't discover interesting things to do with your data unless you can ask arbitrary questions of it.</p>
<h4 id="minimal-maintenance">Minimal maintenance<a class="headerlink" href="#minimal-maintenance" title="Permanent link">&para;</a></h4>
<p>Maintenance is the work required to keep a system running smoothly. This includes:</p>
<ul>
<li>Anticipating when to add machines to scale,</li>
<li>Keeping processes up and running,</li>
<li>Debugging anything that goes wrong in production.</li>
</ul>
<p>An important part of minimizing maintenance is choosing components that have as little implementation complexity as possible. You want to rely on components that have simple mechanisms underlying them. In particular, distributed databases tend to have very complicated internals. The more complex a system, the more likely something will go wrong, and the more you need to understand about the system to debug and tune it.</p>
<p><u>You combat implementation complexity by relying on simple algorithms and simple components.</u> A trick employed in the Lambda Architecture is to push complexity out of the core components and into pieces of the system whose outputs are discardable after a few hours. The most complex components used, like read/write distributed databases, are in this layer where outputs are eventually discardable.</p>
<h4 id="debuggability">Debuggability<a class="headerlink" href="#debuggability" title="Permanent link">&para;</a></h4>
<p>A Big Data system must provide the information necessary to debug the system when things go wrong. The key is to be able to trace, for each value in the system, exactly what caused it to have that value.</p>
<p>Debuggability is accomplished in the Lambda Architecture through the functional nature of the batch layer and by preferring to use recomputation algorithms when possible.</p>
<h3 id="the-problems-with-fully-incremental-architectures">The problems with fully incremental architectures<a class="headerlink" href="#the-problems-with-fully-incremental-architectures" title="Permanent link">&para;</a></h3>
<p>Traditional architectures look like the figure below:</p>
<p><a href="../figure_1.3.png" title="Figure 1.3 Fully incremental architecture"><img alt="Figure 1.3 Fully incremental architecture" src="../figure_1.3.png" /></a></p>
<p>What characterizes these architectures is the use of read/write databases and maintaining the state in those databases incrementally as new data is seen. For example, an incremental approach to counting pageviews would be to process a new pageview by adding one to the counter for its URL. The vast majority of both relational and non-relational database deployments are done as fully incremental architectures. This has been true for many decades.</p>
<p>Fully incremental architectures are so widespread that many people don't realize it's possible to avoid their problems with a different architecture.  This is called <em>familiar complexity</em> (complexity that's so ingrained, you don't even think to find a way to avoid it).</p>
<p>The problems with fully incremental architectures are significant. This section discusses:</p>
<ul>
<li>General complexities brought on by any fully incremental architecture.</li>
<li>Two contrasting solutions for the same problem: one using the best possible fully incremental solution, and one using a Lambda Architecture.</li>
</ul>
<p>You'll see that the fully incremental version is significantly worse in every respect.</p>
<h4 id="operational-complexity">Operational complexity<a class="headerlink" href="#operational-complexity" title="Permanent link">&para;</a></h4>
<p>With many complexities inherent in fully incremental architectures that create difficulties in operating production infrastructure, this section focuses on one: the need for read/write databases to perform online compaction, and what you have to do operationally to keep things running smoothly.</p>
<p>In a read/write database, as a disk index is incrementally added to and modified, parts of the index become unused. These unused parts take up space and eventually need to be reclaimed to prevent the disk from filling up. Reclaiming space as soon as it becomes unused is too expensive, so the space is occasionally reclaimed in bulk in a process called <strong>compaction</strong>.</p>
<p>Compaction is an intensive operation. The server places substantially higher demand on the CPU and disks during compaction, which dramatically lowers the performance of that machine during that time period. Databases such as HBase and Cassandra are well-known for requiring careful configuration and management to avoid problems or server lockups during compaction. The performance loss during compaction is a complexity that can even cause cascading failure: if too many machines compact at the same time, the load they were supporting will have to be handled by other machines in the cluster. This can potentially overload the rest of your cluster, causing total failure.</p>
<p>To manage compaction correctly, you have to:</p>
<ul>
<li>Schedule compactions on each node so that not too many nodes are affected at once.</li>
<li>Be aware of how long a compaction takes to avoid having more nodes undergoing compaction than you intended.</li>
<li>Make sure you have enough disk capacity on your nodes to last them between compactions.</li>
<li>Make sure you have enough capacity on your cluster so that it doesn't become overloaded when resources are lost during compactions.</li>
</ul>
<p>The best way to deal with complexity is to get rid of that complexity altogether. The fewer failure modes you have in your system, the less likely it is that you'll suffer unexpected downtime. Dealing with online compaction is a complexity inherent to fully incremental architectures, but in a Lambda Architecture the primary databases don't require any online compaction.</p>
<h4 id="extreme-complexity-of-achieving-eventual-consistency">Extreme complexity of achieving eventual consistency<a class="headerlink" href="#extreme-complexity-of-achieving-eventual-consistency" title="Permanent link">&para;</a></h4>
<p>Incremental architectures have another complexity when trying to make the system highly available.  A highly available system allows for queries and updates even in the presence of machine or partial network failure.</p>
<p>Achieving high availability competes directly with another important property called <a href="https://en.wikipedia.org/wiki/Consistency_(database_systems)"><strong>consistency</strong></a>. A consistent system returns results that take into account all previous writes. The <a href="https://en.wikipedia.org/wiki/CAP_theorem">CAP theorem</a> has shown that it's impossible to achieve both high availability and consistency in the same system in the presence of network partitions. Therefore, a highly available system sometimes returns stale results during a network partition.</p>
<p>In order for a highly available system to return to consistency once a network partition ends (known as <a href="https://en.wikipedia.org/wiki/Eventual_consistency"><strong>eventual consistency</strong></a>), a lot of help is required from your application. [p11] Distributed databases achieve high availability by keeping multiple replicas of all information stored. When you keep many copies of the same information, that information is still available even if a machine goes down or the network gets partitioned, as shown in the figure below. During a network partition, a system that chooses to be highly available has clients update whatever replicas are reachable to them. This causes replicas to diverge and receive different sets of updates. Only when the partition goes away can the replicas be merged together into a common value.</p>
<p><a href="../figure_1.4.png" title="Figure 1.4 Using replication to increase availability"><img alt="Figure 1.4 Using replication to increase availability" src="../figure_1.4_600.png" /></a></p>
<h5 id="example-highly-available-counting"><strong>Example: highly available counting</strong> *<a class="headerlink" href="#example-highly-available-counting" title="Permanent link">&para;</a></h5>
<p>For example, suppose you have two replicas with a count of 10 when a network partition begins. Suppose the first replica gets two increments and the second gets one increment.  When it comes time to merge these replicas together, with values of 12 and 11, what should the merged value be? Although the correct answer is 13, there's no way to know just by looking at the numbers 12 and 11. They could have diverged at 11 (in which case the answer would be 12), or they could have diverged at 0 (in which case the answer would be 23).</p>
<p>To do highly available counting correctly, it's not enough to just store a count:</p>
<ul>
<li>You need a data structure that's amenable to merging when values diverge,</li>
<li>You need to implement the code that will repair values once partitions end.</li>
</ul>
<p>This is an amazing amount of complexity you have to deal with just to maintain a simple count.</p>
<p>In general, handling eventual consistency in incremental, highly available systems is unintuitive and prone to error. This complexity is innate to highly available, fully incremental systems. However, the Lambda Architecture structures itself in a different way that greatly lessens the burdens of achieving highly available, eventually consistent systems.</p>
<h4 id="lack-of-human-fault-tolerance">Lack of human-fault tolerance<a class="headerlink" href="#lack-of-human-fault-tolerance" title="Permanent link">&para;</a></h4>
<p>The last problem with fully incremental architectures is their inherent lack of human-fault tolerance. <u>An incremental system is constantly modifying the state it keeps in the database, which means a mistake can also modify the state in the database. Because mistakes are inevitable, the database in a fully incremental architecture is guaranteed to be corrupted.</u></p>
<p>This is one of the few complexities of fully incremental architectures that can be resolved without a complete rethinking of the architecture. Consider the two architectures shown in the following figure:</p>
<ul>
<li>Synchronous architecture, where the application makes updates directly to the database.</li>
<li>Asynchronous architecture, where events go to a queue before updating the database in the background.</li>
</ul>
<p><a href="../figure_1.5.png" title="Figure 1.5 Adding logging to fully incremental architectures"><img alt="Figure 1.5 Adding logging to fully incremental architectures" src="../figure_1.5.png" /></a></p>
<p>In both cases, every event is permanently logged to an events datastore. By keeping every event, if a human mistake causes database corruption, you can go back to the events store and reconstruct the proper state for the database. Because the events store is immutable and constantly growing, redundant checks, like permissions, can be put in to make it highly unlikely for a mistake to trample over the events store. This technique is also core to the Lambda Architecture and is discussed in depth in <a href="../ch2/">Chapter 2</a> and <a href="../ch3/">Chapter 3</a>.</p>
<p>Although fully incremental architectures with logging can overcome the human-fault tolerance deficiencies of those without logging, the logging cannot handle the other complexities that have been discussed.</p>
<h4 id="fully-incremental-solution-vs-lambda-architecture-solution">Fully incremental solution vs. Lambda Architecture solution<a class="headerlink" href="#fully-incremental-solution-vs-lambda-architecture-solution" title="Permanent link">&para;</a></h4>
<p>One of the example queries implemented throughout the book serves as a great contrast between fully incremental and Lambda architectures. The query has to do with pageview analytics and is done on two kinds of data coming in:</p>
<ul>
<li><em>Pageviews</em>, which contain a user ID, URL, and timestamp.</li>
<li><em>Equivs</em>, which contain two user IDs. An equiv indicates the two user IDs refer to the same person.</li>
</ul>
<p>The goal of the query is to compute the number of unique visitors to a URL over a
range of time. Queries should be up to date with all data and respond with minimal
latency (less than 100 milliseconds). Below is the interface for the query:</p>
<div class="codehilite"><pre><span class="kt">long</span> <span class="nf">uniquesOverTime</span><span class="o">(</span><span class="n">String</span> <span class="n">url</span><span class="o">,</span> <span class="kt">int</span> <span class="n">startHour</span><span class="o">,</span> <span class="kt">int</span> <span class="n">endHour</span><span class="o">)</span>
</pre></div>


<p>If a person visits the same URL in a time range with two user IDs connected via equivs (even transitively), that should only count as one visit. A new equiv coming in can change the results for any query over any time range for any URL.</p>
<p>Instead of showing details of the solutions which require covering many concepts such as indexing, distributed databases, batch processing, <a href="https://en.wikipedia.org/wiki/HyperLogLog">HyperLogLog</a>, we'll focus on the characteristics of the solutions and the striking differences between them. The best possible fully incremental solution is shown in detail in Chapter 10, and the Lambda Architecture solution is built up in Chapter 8, 9, 14, and 15.</p>
<p>The two solutions can be compared on three axes: accuracy, latency, and throughput. [p14] The Lambda Architecture solution is significantly better in all respects. Lambda Architecture can produce solutions with higher performance in every respect, while also avoiding the complexity that plagues fully incremental architectures.</p>
<h3 id="lambda-architecture">Lambda Architecture<a class="headerlink" href="#lambda-architecture" title="Permanent link">&para;</a></h3>
<p>Computing arbitrary functions on an arbitrary dataset in real time is not a simple problem. There's no single tool that provides a complete solution. Instead, you have to use a variety of tools and techniques to build a complete Big Data system.</p>
<p>The main idea of the Lambda Architecture is to build Big Data systems as a series of layers, as shown in the following figure.</p>
<p><a href="../figure_1.6.png" title="Figure 1.6 Lambda Architecture"><img alt="Figure 1.6 Lambda Architecture" src="../figure_1.6.png" /></a></p>
<p>Each layer satisfies a subset of the properties and builds upon the functionality provided by the layers beneath it. Each layer requires a lot of work to design, implement, and deploy, but the high-level ideas of the whole system are easy to understand.</p>
<p>Starting everything from the <em>query</em> = <em>function</em>(<em>all data</em>) equation, you could ideally run the functions on the fly to get the results.  However, this would take a huge amount of resources to do and would be unreasonably expensive. This is similar to having to read a petabyte dataset every time you wanted to answer the query of someone's current location.</p>
<p>The most obvious alternative approach is to precompute the query function, which is called the <em>batch view</em>. Instead of computing the query on the fly, you read the results from the precomputed view. The precomputed view is indexed so that it can be accessed with random reads:</p>
<blockquote>
<p>batch view = function(all data)</p>
<p>query = function(batch view)</p>
</blockquote>
<p>This system works as follows:</p>
<ol>
<li>Run a function on all the data to get the batch view.</li>
<li>When you want to know the value for a query, run a function on that batch view.</li>
<li>The batch view makes it possible to get the values you need from it very quickly, without having to scan everything in it.</li>
</ol>
<p>For example, you're building a web analytics application, and you want to query the number of pageviews for a URL on any range of days. If you were computing the query as a function of all the data, you'd scan the dataset for pageviews for that URL within that time range, and return the count of those results.</p>
<p>Instead, the batch view approach (as show in the figure below) works as follows:</p>
<ol>
<li>Run a function on all the pageviews to precompute an index from a key of <code>[url, day]</code> to the count of the number of pageviews for that URL for that day.</li>
<li>To resolve the query, retrieve all values from that view for all days within that time range, and sum up the counts to get the result.</li>
</ol>
<p><a href="../figure_1.7.png" title="Figure 1.7 Architecture of the batch layer"><img alt="Figure 1.7 Architecture of the batch layer" src="../figure_1.7.png" /></a></p>
<p>Creating the batch view (with this approach described so far) is a high-latency operation, because it's running a function on all the data you have. By the time it finishes, a lot of new data will have collected that's not represented in the batch views, and the queries will be out of date by many hours. We will ignore this issue for the moment (because we'll be able to fix it) and assume it's fine for queries to be out of date by a few hours and continue exploring this idea of precomputing a batch view by running a function on the complete dataset.</p>
<h4 id="batch-layer">Batch layer<a class="headerlink" href="#batch-layer" title="Permanent link">&para;</a></h4>
<p>The <strong>batch layer</strong> is the portion of the Lambda Architecture that implements the <em>batch view</em> = <em>function</em>(<em>all data</em>) equation.</p>
<p>The batch layer stores the master copy of the dataset and precomputes batch views on that master dataset, as show in the figure below. The master dataset can be thought of as a very large list of records.</p>
<p><a href="../figure_1.8.png" title="Figure 1.8 Batch layer"><img alt="Figure 1.8 Batch layer" src="../figure_1.8.png" /></a></p>
<p>The batch layer needs to be able to do two things:</p>
<ol>
<li>Store an immutable, constantly growing master dataset.</li>
<li>Compute arbitrary functions on that dataset.</li>
</ol>
<p>This type of processing is best done using batch-processing systems. Hadoop is the canonical example of a batch-processing system.</p>
<p>The batch layer can be represented in pseudo-code like this:</p>
<div class="codehilite"><pre>function runBatchLayer():
  while(true):
    recomputeBatchViews()
</pre></div>


<p>The batch layer runs in a <code>while(true)</code> loop and continuously recomputes the batch views from scratch. This is the best way to think about the batch layer at the moment, though in reality, the batch layer is a little more involved.</p>
<p>The batch layer is simple to use:</p>
<ul>
<li>Batch computations are written like single-threaded programs, and you get parallelism for free.</li>
<li>It's easy to write robust, highly scalable computations on the batch layer.</li>
<li>The batch layer scales by adding new machines.</li>
</ul>
<p>The following is an example of a batch layer computation. You don't have to understand this code; the point is to show what an inherently parallel program looks like:</p>
<div class="codehilite"><pre><span class="n">Api</span><span class="o">.</span><span class="na">execute</span><span class="o">(</span><span class="n">Api</span><span class="o">.</span><span class="na">hfsSeqfile</span><span class="o">(</span><span class="s">&quot;/tmp/pageview-counts&quot;</span><span class="o">),</span>
    <span class="k">new</span> <span class="n">Subquery</span><span class="o">(</span><span class="s">&quot;?url&quot;</span><span class="o">,</span> <span class="s">&quot;?count&quot;</span><span class="o">)</span>
        <span class="o">.</span><span class="na">predicate</span><span class="o">(</span><span class="n">Api</span><span class="o">.</span><span class="na">hfsSeqfile</span><span class="o">(</span><span class="s">&quot;/data/pageviews&quot;</span><span class="o">),</span>
            <span class="s">&quot;?url&quot;</span><span class="o">,</span> <span class="s">&quot;?user&quot;</span><span class="o">,</span> <span class="s">&quot;?timestamp&quot;</span><span class="o">)</span>
        <span class="o">.</span><span class="na">predicate</span><span class="o">(</span><span class="k">new</span> <span class="n">Count</span><span class="o">(),</span> <span class="s">&quot;?count&quot;</span><span class="o">);)</span>
</pre></div>


<p>This code computes the number of pageviews for every URL given an input dataset of raw pageviews:</p>
<ul>
<li>All the concurrency challenges of scheduling work and merging results is done for you.</li>
<li>Because the algorithm is written in this way, it can be arbitrarily distributed on a MapReduce cluster, scaling to however many nodes you have available.</li>
<li>At the end of the computation, the output directory will contain some number of files with the results.</li>
</ul>
<h4 id="serving-layer">Serving layer<a class="headerlink" href="#serving-layer" title="Permanent link">&para;</a></h4>
<p>The serving layer is a specialized distributed database that loads in a batch view (emitted by the batch layer as the result of its functions) and makes it possible to do random reads on it, as seen in the following figure. When new batch views are available, the serving layer automatically swaps those in so that more up-to-date results are available.</p>
<p><a href="../figure_1.9.png" title="Figure 1.9 Serving layer"><img alt="Figure 1.9 Serving layer" src="../figure_1.9.png" /></a></p>
<p>A serving layer database supports batch updates and random reads, but it doesn't need to support random writes. This is a very important point, as random writes cause most of the complexity in databases. By not supporting random writes, these databases are extremely simple. That simplicity makes them robust, predictable, easy to configure, and easy to operate. ElephantDB, the serving layer database discussed in this book, is only a few thousand lines of code.</p>
<h4 id="batch-and-serving-layers-satisfy-almost-all-properties">Batch and serving layers satisfy almost all properties<a class="headerlink" href="#batch-and-serving-layers-satisfy-almost-all-properties" title="Permanent link">&para;</a></h4>
<p>The batch and serving layers support arbitrary queries on an arbitrary dataset with the trade-off that queries will be out of date by a few hours. It takes a new piece of data a few hours to propagate through the batch layer into the serving layer where it can be queried. Other than low latency updates, the batch and serving layers satisfy every property desired in a Big Data system, as outlined in <a href="#desired-properties-of-a-big-data-system">Section 1.5</a>:</p>
<ul>
<li><strong>Robustness and fault tolerance</strong>.<ul>
<li>Hadoop handles failover when machines go down.</li>
<li>The serving layer uses replication to ensure availability when servers go down.</li>
<li>The batch and serving layers are also human-fault tolerant, because when a mistake is made, you can fix your algorithm or remove the bad data and recompute the views from scratch.</li>
</ul>
</li>
<li><strong>Scalability</strong>. Both the batch and serving layers are easily scalable. They're both fully distributed systems, and scaling them is as easy as adding new machines.</li>
<li><strong>Generalization</strong>. You can compute and update arbitrary views of an arbitrary dataset.</li>
<li><strong>Extensibility</strong>.<ul>
<li>Adding a new view is as easy as adding a new function of the master dataset. Because the master dataset can contain arbitrary data, new types of data can be easily added.</li>
<li>If you want to tweak a view, you don't have to worry about supporting multiple versions of the view in the application. You can simply recompute the entire view from scratch.</li>
</ul>
</li>
<li><strong>Ad hoc queries</strong>. The batch layer supports ad hoc queries innately. All the data is conveniently available in one location.</li>
<li><strong>Minimal maintenance</strong>.<ul>
<li>The main component to maintain in this system is Hadoop. Hadoop requires some administration knowledge, but it's fairly straightforward to operate.</li>
<li>The serving layer databases are simple because they don't do random writes. Because a serving layer database has so few moving parts, there's lots less that can go wrong. As a consequence, it's much less likely that anything will go wrong with a serving layer database, so they're easier to maintain.</li>
</ul>
</li>
<li><strong>Debuggability</strong>. Having the inputs and outputs of computations run on the batch layer gives you all the information you need to debug when something goes wrong.<ul>
<li>In a traditional database, an output can replace the original input (such as when incrementing a value). In the batch and serving layers, the input is the master dataset and the output is the views. Likewise, you have the inputs and outputs for all the intermediate steps.</li>
</ul>
</li>
</ul>
<p>The batch and serving layers satisfy almost all the properties you want with a simple and easy-to-understand approach. There are no concurrency issues to deal with, and it scales trivially. The only property missing is low latency updates. The final layer, the speed layer, fixes this problem.</p>
<h3 id="speed-layer">Speed layer<a class="headerlink" href="#speed-layer" title="Permanent link">&para;</a></h3>
<p>Since the serving layer updates whenever the batch layer finishes precomputing a batch view, the only data not represented in the batch view is the data that came in while the precomputation was running. The purpose of the speed layer is to compensate for those last few hours of data, in order to have arbitrary functions computed on arbitrary data in real time. The goal of the speed layer is to ensure new data is represented in query functions as quickly as needed for the application requirements (see figure below).</p>
<p><a href="../figure_1.10.png" title="Figure 1.10 Speed layer"><img alt="Figure 1.10 Speed layer" src="../figure_1.10.png" /></a></p>
<p>The speed layer is similar to the batch layer in that it produces views based on data it receives, but has two major differences:</p>
<ol>
<li>The speed layer only looks at recent data, whereas the batch layer looks at all the data at once.</li>
<li>In order to achieve the smallest latencies possible, the speed layer doesn't look at all the new data at once. Instead, it updates the realtime views as it receives new data instead of recomputing the views from scratch like the batch layer does. The speed layer does incremental computation instead of the recomputation done in the batch layer.</li>
</ol>
<p>The data flow on the speed layer can be formalized with the following equation:</p>
<blockquote>
<p>realtime view = function(realtime view, new data)</p>
</blockquote>
<p>A realtime view is updated based on new data and the existing realtime view.</p>
<p>The Lambda Architecture in full is summarized by these three equations:</p>
<blockquote>
<p>batch view = function(all data)</p>
<p>realtime view = function(realtime view, new data)</p>
<p>query = function(batch view, realtime view)</p>
</blockquote>
<p>A pictorial representation of these ideas is shown in the figure below:</p>
<p><a href="../figure_1.11.png" title="Figure 1.11 Lambda Architecture diagram"><img alt="Figure 1.11 Lambda Architecture diagram" src="../figure_1.11.png" /></a></p>
<p>Instead of resolving queries by just doing a function of the batch view, you resolve queries by looking at both the batch and realtime views and merging the results together.</p>
<p>The speed layer uses databases that support random reads and random writes, so they're orders of magnitude more complex than the databases in the serving layer, both in terms of implementation and operation. However, <u>once data makes it through the batch layer into the serving layer, the corresponding results in the realtime views are no longer needed.</u> This means you can discard pieces of the realtime view as they're no longer needed. This is a wonderful result, because the speed layer is far more complex than the batch and serving layers. This property of the Lambda Architecture is called <em>complexity isolation</em>, meaning that complexity is pushed into a layer whose results are only temporary. If anything ever goes wrong, you can discard the state for the entire speed layer, and everything will be back to normal within a few hours.</p>
<h4 id="example-of-the-web-analytics-application-continued">Example of the web analytics application (continued) *<a class="headerlink" href="#example-of-the-web-analytics-application-continued" title="Permanent link">&para;</a></h4>
<p>To continue the example of building a web analytics application that supports queries about the number of pageviews over a range of days, recall that the batch layer produces batch views from <code>[url, day]</code> to the number of pageviews. The speed layer keeps its own separate view of <code>[url, day]</code> to number of pageviews.  Whereas the batch layer recomputes its views by literally counting the pageviews, the speed layer updates its views by incrementing the count in the view whenever it receives new data. To resolve a query, you query both the batch and realtime views as necessary to satisfy the range of dates specified, and you sum up the results to get the final count. The little work that needs to be done to properly synchronize the results will be covered in a future chapter.</p>
<h4 id="eventual-accuracy">Eventual accuracy *<a class="headerlink" href="#eventual-accuracy" title="Permanent link">&para;</a></h4>
<p>Some algorithms are difficult to compute incrementally. The batch/speed layer split gives you the flexibility to use the exact algorithm on the batch layer and an approximate algorithm on the speed layer. The batch layer repeatedly overrides (corrects) the speed layer, so the approximation gets corrected and your system exhibits the property of <em>eventual accuracy</em>. For exmple, computing unique counts can be challenging if the sets of uniques get large. It's easy to do the unique count on the batch layer, because you look at all the data at once, but on the speed layer you might use a <a href="https://en.wikipedia.org/wiki/HyperLogLog">HyperLogLog</a> set as an approximation.</p>
<h4 id="performance-and-robustness">Performance and robustness *<a class="headerlink" href="#performance-and-robustness" title="Permanent link">&para;</a></h4>
<p>The resulting system has both performance and robustness. [p20] You can still get low latency updates, the complexity of achieving this doesn't affect the robustness of your results because the speed layer is transient. <u>The transient nature of the speed layer gives you the flexibility to be very aggressive when it comes to making trade-offs for performance.</u> For computations that can be done exactly in an incremental fashion, the system is fully accurate.</p>
<h3 id="recent-trends-in-technology">Recent trends in technology<a class="headerlink" href="#recent-trends-in-technology" title="Permanent link">&para;</a></h3>
<p>A number of trends in technology deeply influence the ways to build Big Data systems.</p>
<h4 id="cpus-arent-getting-faster">CPUs aren't getting faster<a class="headerlink" href="#cpus-arent-getting-faster" title="Permanent link">&para;</a></h4>
<p>CPUs hit the physical limits of speed. That means that if you want to scale to more data, you must be able to parallelize your computation.  This has led to the rise of shared-nothing parallel algorithms and their corresponding systems, such as MapReduce. Instead of just trying to scale by buying a better machine, known as <strong>vertical scaling</strong>, systems scale by adding more machines, known as <strong>horizontal scaling</strong>.</p>
<h4 id="elastic-clouds">Elastic clouds<a class="headerlink" href="#elastic-clouds" title="Permanent link">&para;</a></h4>
<p>Elastic clouds, also known as <a href="https://en.wikipedia.org/wiki/Cloud_computing#Infrastructure_as_a_service_.28IaaS.29">Infrastructure as a Service</a> (IaaS), is another trend in technology. <a href="https://en.wikipedia.org/wiki/Amazon_Web_Services">Amazon Web Services</a> (AWS) is the most notable elastic cloud. Elastic clouds allow you to:</p>
<ul>
<li>Rent hardware on demand rather than own your own hardware in your own location.</li>
<li>Increase or decrease the size of your cluster nearly instantaneously. If you have a big job you want to run, you can allocate the hardware temporarily.</li>
</ul>
<p>Elastic clouds has the following advantages:</p>
<ul>
<li>Dramatically simplify system administration.</li>
<li>Provide additional storage and hardware allocation options that can significantly drive down the price of infrastructure.<ul>
<li>For example, AWS has a feature called <a href="https://aws.amazon.com/ec2/spot/">spot instances</a> in which you bid on instances rather than pay a fixed price. If someone bids a higher price than you, you'll lose the instance. Because spot instances can disappear at any moment, they tend to be significantly cheaper than normal instances.</li>
</ul>
</li>
<li>Fault tolerance is handled at the software layer for distributed computation systems like MapReduce.</li>
</ul>
<h4 id="vibrant-open-source-ecosystem-for-big-data">Vibrant open source ecosystem for Big Data<a class="headerlink" href="#vibrant-open-source-ecosystem-for-big-data" title="Permanent link">&para;</a></h4>
<p>There are five categories of open source projects under discussion.</p>
<ul>
<li><strong>Batch computation systems</strong>. Batch computation systems are high throughput, high latency systems.<ul>
<li>Batch computation systems can do nearly arbitrary computations, but they may take hours or days to do so.</li>
<li>The only batch computation system discussed is Hadoop. The Hadoop project has two subprojects: Hadoop Distributed File System (HDFS) and Hadoop MapReduce.<ul>
<li>HDFS is a distributed, fault-tolerant storage system that can scale to petabytes of data.</li>
<li>MapReduce is a horizontally scalable computation framework that integrates with HDFS.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Serialization frameworks</strong>. Serialization frameworks provide tools and libraries for using objects between languages. They can:</p>
<ul>
<li>Serialize an object into a byte array from any language, and then deserialize that byte array into an object in any language.</li>
<li>Provide a Schema Definition Language for defining objects and their fields,</li>
<li>Provide mechanisms to safely version objects so that a schema can be evolved without invalidating existing objects.</li>
</ul>
<p>The three notable serialization frameworks are <a href="https://en.wikipedia.org/wiki/Apache_Thrift">Thrift</a>, <a href="https://en.wikipedia.org/wiki/Protocol_Buffers">Protocol Buffers</a>, and <a href="https://en.wikipedia.org/wiki/Apache_Avro">Avro</a>.</p>
</li>
<li>
<p><strong>Random-access NoSQL databases</strong>. There has been a plethora of NoSQL databases created in the past few years, such as <a href="https://en.wikipedia.org/wiki/Apache_Cassandra">Cassandra</a>, <a href="https://en.wikipedia.org/wiki/Apache_HBase">HBase</a>, <a href="https://en.wikipedia.org/wiki/MongoDB">MongoDB</a>, <a href="https://en.wikipedia.org/wiki/Voldemort_(distributed_data_store)">Voldemort</a>, <a href="https://en.wikipedia.org/wiki/Riak">Riak</a>, <a href="https://en.wikipedia.org/wiki/CouchDB">CouchDB</a>.</p>
<ul>
<li>These databases all share one thing in common: they sacrifice the full expressiveness of SQL and instead specialize in certain kinds of operations.</li>
<li>They all have different semantics and are meant to be used for specific purposes. They're not meant to be used for arbitrary <a href="https://en.wikipedia.org/wiki/Data_warehouse">data warehousing</a>. <u>Choosing a NoSQL database to use is like choosing between a hash map, sorted map, linked list, or vector when choosing a data structure to use in a program.</u> Cassandra will be used as part of the example application to be discussed.</li>
</ul>
</li>
<li><strong>Messaging/queuing systems</strong>. A messaging/queuing system provides a way to send and consume messages between processes in a fault-tolerant and asynchronous manner. A message queue is a key component for doing realtime processing. <a href="https://en.wikipedia.org/wiki/Apache_Kafka">Apache Kafka</a> is discussed in this book.</li>
<li><strong>Realtime computation system</strong>. Realtime computation systems are high throughput, low latency, stream-processing systems. They can't do the range of computations a batch-processing system can, but they process messages extremely quickly. <a href="https://en.wikipedia.org/wiki/Storm_(event_processor)">Storm</a> is used in this book. Storm topologies are easy to write and scale.</li>
</ul>
<p>As these open source projects have matured, companies have formed around some of
them to provide enterprise support. For example:</p>
<p>Enterprise support:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Cloudera">Cloudera</a> provides Hadoop support</li>
<li><a href="https://en.wikipedia.org/wiki/DataStax">DataStax</a> provides Cassandra support</li>
</ul>
<p>Company products:</p>
<ul>
<li>Riak is a product of <a href="https://en.wikipedia.org/wiki/Basho_Technologies">Basho Technologies</a></li>
<li>MongoDB is a product of <a href="https://en.wikipedia.org/wiki/MongoDB_Inc.">MongoDB Inc.</a> (formerly 10gen)</li>
<li><a href="https://en.wikipedia.org/wiki/RabbitMQ">RabbitMQ</a> is a product of <a href="https://en.wikipedia.org/wiki/SpringSource">SpringSource</a>, a division of VMWare.</li>
</ul>
<h3 id="example-application-superwebanalyticscom">Example application: SuperWebAnalytics.com<a class="headerlink" href="#example-application-superwebanalyticscom" title="Permanent link">&para;</a></h3>
<p>This book discusses building an example Big Data application. The data management layer is built for a Google Analytics–like service.  The service will be able to track billions of pageviews per day.</p>
<p>The service will support a variety of different metrics. Each metric will be supported in real time. The metrics range from simple counting metrics to complex analyses of how visitors are navigating a website.</p>
<p>The following metrics are to be supported:</p>
<ul>
<li><strong>Pageview counts by URL sliced by time.</strong><ul>
<li>"What are the pageviews for each day over the past year?"</li>
<li>"How many pageviews have there been in the past 12 hours?"</li>
</ul>
</li>
<li><strong>Unique visitors by URL sliced by time.</strong><ul>
<li>How many unique people visited this domain in 2010?"</li>
<li>"How many unique people visited this domain each hour for the past three days?"</li>
</ul>
</li>
<li><strong>Bounce-rate analysis.</strong><ul>
<li>"What percentage of people visit the page without visiting any other pages on this website?"</li>
</ul>
</li>
</ul>
<p>The layers that store, process, and serve queries to the application will be build out.</p>
<h3 id="summary">Summary<a class="headerlink" href="#summary" title="Permanent link">&para;</a></h3>
<p>[p23]</p>
<p>The problems of scaling a relational system with traditional techniques (e.g. sharding) can be beyond scaling itself as the system becomes more complex to manage, extend, and even understand. The upcoming chapters focus as much on robustness as on scalability, and shows that when building things the right way, both robustness and scalability are achievable in the same system.</p>
<p>The benefits of data systems built using the Lambda Architecture is not only scaling:</p>
<ul>
<li>More data and more value out of it can be collected. Increasing the amount and types of data you store will lead to more opportunities to mine your data, produce analytics, and build new applications.</li>
<li>How robust your applications will be. There are many reasons for this, for example:<ul>
<li>You'll have the ability to run computations on your whole dataset to do migrations or fix things that go wrong.</li>
<li>You'll never have to deal with situations where there are multiple versions of a schema active at the same time.</li>
<li>When you change your schema, you'll have the capability to update all data to the new schema.</li>
<li>If an incorrect algorithm is accidentally deployed to production and corrupts the data you're serving, you can easily fix things by recomputing the corrupted values.</li>
</ul>
</li>
<li>Performance will be more predictable. Although the Lambda Architecture as a whole is generic and flexible, the individual components comprising the system are specialized. There is very little "magic" happening behind the scenes, as compared to something like a <a href="https://en.wikipedia.org/wiki/Query_plan">SQL query planner</a>.</li>
</ul>
<p>The next chapter discusses how to build the Lambda Architecture. You'll start at the very core of the stack with how you model and schematize the master copy of your dataset.</p>
<h3 id="doubts-and-solutions">Doubts and Solutions<a class="headerlink" href="#doubts-and-solutions" title="Permanent link">&para;</a></h3>
<h4 id="verbatim">Verbatim<a class="headerlink" href="#verbatim" title="Permanent link">&para;</a></h4>
<h5 id="p10-on-operational-complexity"><strong>p10 on Operational complexity</strong><a class="headerlink" href="#p10-on-operational-complexity" title="Permanent link">&para;</a></h5>
<blockquote>
<p>In a read/write database, as a disk index is incrementally added to and modified, parts of the index become unused. These unused parts take up space and eventually need to be reclaimed to prevent the disk from filling up. Reclaiming space as soon as it becomes unused is too expensive, so the space is occasionally reclaimed in bulk in a process called <em>compaction</em>.</p>
</blockquote>
<p><span class="text-danger">Question</span>: What is a disk index?</p>
            </div>
        </div>

        <footer class="col-md-12">
            
        </footer>

        <script src="../../js/jquery-1.10.2.min.js"></script>
        <script src="../../js/bootstrap-3.0.3.min.js"></script>
        <script src="../../js/highlight.pack.js"></script>
        <script src="../../js/base.js"></script>
        <script src="../../custom.js"></script>
    </body>
</html>